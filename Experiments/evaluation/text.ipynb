{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.55, 0.528, 0.539)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Multi/gpt-4.1-2025-04-14/S_Multi_gpt-4.1-2025-04-14_-1_1747180230.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Uni/gpt-4.1-2025-04-14/S_Uni_gpt-4.1-2025-04-14_-1_1747176858.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7126666666666667, 0.572, 0.6423333333333333)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Multi/gpt-4.1-2025-04-14/S_Multi_gpt-4.1-2025-04-14_10_1747172694.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Uni/gpt-4.1-2025-04-14/S_Uni_gpt-4.1-2025-04-14_10_1747178716.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.988, 0.682, 0.835)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Multi/gpt-4.1-2025-04-14/S_Multi_gpt-4.1-2025-04-14_1_1747168253.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Uni/gpt-4.1-2025-04-14/S_Uni_gpt-4.1-2025-04-14_1_1747170200.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11385555555555556, 0.152, 0.13292777777777778)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Multi/gpt-4.1-2025-04-14/A_Multi_gpt-4.1-2025-04-14_-1_1747211775.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Uni/gpt-4.1-2025-04-14/A_Uni_gpt-4.1-2025-04-14_-1_1747219381.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20287777777777777, 0.221, 0.2119388888888889)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Multi/gpt-4.1-2025-04-14/A_Multi_gpt-4.1-2025-04-14_10_1747211346.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Uni/gpt-4.1-2025-04-14/A_Uni_gpt-4.1-2025-04-14_10_1747218001.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9889111111111111, 0.994, 0.9914555555555555)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Multi/gpt-4.1-2025-04-14/A_Multi_gpt-4.1-2025-04-14_1_1747210873.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Uni/gpt-4.1-2025-04-14/A_Uni_gpt-4.1-2025-04-14_1_1747216665.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15348888888888887, 0.032, 0.09274444444444443)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Multi/gpt-4.1-2025-04-14/T_Multi_gpt-4.1-2025-04-14_-1_1747175876.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Uni/gpt-4.1-2025-04-14/T_Uni_gpt-4.1-2025-04-14_-1_1747173955.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23256666666666667, 0.214, 0.22328333333333333)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Multi/gpt-4.1-2025-04-14/T_Multi_gpt-4.1-2025-04-14_10_1747176371.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Uni/gpt-4.1-2025-04-14/T_Uni_gpt-4.1-2025-04-14_10_1747161891.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9427444444444444, 0.948, 0.9453722222222222)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Multi/gpt-4.1-2025-04-14/T_Multi_gpt-4.1-2025-04-14_1_1747168216.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Uni/gpt-4.1-2025-04-14/T_Uni_gpt-4.1-2025-04-14_1_1747159868.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.38999999999999"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 Token counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Dataset_Generation/Data/F_Uni.jsonl\n",
      "../../Dataset_Generation/Data/A_Uni.jsonl\n",
      "../../Dataset_Generation/Data/T_Multi.jsonl\n",
      "../../Dataset_Generation/Data/T_Uni.jsonl\n",
      "../../Dataset_Generation/Data/F_Multi.jsonl\n",
      "../../Dataset_Generation/Data/A_Multi.jsonl\n",
      "Loaded 6 files\n",
      "Total entries: 9000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "# Get all jsonl files in the Dataset_Generation/Data directory\n",
    "jsonl_files = glob.glob('../../Dataset_Generation/Data/*.jsonl')\n",
    "\n",
    "# Dictionary to store data from each file\n",
    "file_data = {}\n",
    "\n",
    "# Process each file\n",
    "for file_path in jsonl_files:\n",
    "    print(file_path)\n",
    "    file_data[file_path] = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                file_data[file_path].append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "print(f\"Loaded {len(file_data)} files\")\n",
    "print(f\"Total entries: {sum(len(entries) for entries in file_data.values())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2024-11-23 09:01, Dante: Hey, how have you been?\\n2024-11-23 09:06, Vivian: I’m doing well, actually! Been busy with some new initiatives at the conservancy, but it's good busy. Lots of little wins. How about you? Anything exciting happening?\\n2024-11-23 09:08, Dante: Things are good. I just got back from quite an intensive period – I was at Saints Clemens and Pancratius Church for an artist-in-residence program. It was… a lot.\\n2024-11-23 09:14, Vivian: Oh, wow! That sounds like it demanded a lot of energy. What kind of work were you doing while you were there?\\n2024-11-23 09:15, Dante: It was primarily about observing the space and letting it seep into my work, experimenting with forms inspired by the architecture. I ended up writing a series of poems reflecting on memory and time. It was challenging, but deeply rewarding.\\n2024-11-23 09:17, Vivian: That’s fascinating. It sounds like a really immersive experience. I imagine a place like that holds a lot of stories.\\n2024-11-23 09:43, Dante: Absolutely. It felt like the walls themselves were whispering tales. It really shifted my perspective on things, you know? I’m trying to process it all now.\\n2024-11-23 09:47, Vivian: I can relate to that – sometimes fully letting a place influence you takes a while to unpack. What are you working on next, now that it's over?\\n2024-11-23 09:53, Dante: I’m hoping to adapt some of the poems into a spoken word performance piece. I have a small open mic night booked next week, so I'm prepping for that. A bit daunting, honestly! \\n2024-11-23 09:58, Vivian: That sounds amazing! You’ll be brilliant, I'm sure. Performing is a brave thing, and you have such a powerful way with words. I’m really looking forward to hearing about how it goes.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "file_data['../../Dataset_Generation/Data/F_Uni.jsonl'][i]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../../Dataset_Generation/Data/F_Uni.jsonl\n",
      "Average context tokens: 471.4\n",
      "Max context tokens: 579\n",
      "\n",
      "Processing ../../Dataset_Generation/Data/A_Uni.jsonl\n",
      "Average context tokens: 553.4\n",
      "Max context tokens: 693\n",
      "\n",
      "Processing ../../Dataset_Generation/Data/T_Multi.jsonl\n",
      "Average context tokens: 141.7\n",
      "Max context tokens: 201\n",
      "\n",
      "Processing ../../Dataset_Generation/Data/T_Uni.jsonl\n",
      "Average context tokens: 479.5\n",
      "Max context tokens: 625\n",
      "\n",
      "Processing ../../Dataset_Generation/Data/F_Multi.jsonl\n",
      "Average context tokens: 168.9\n",
      "Max context tokens: 230\n",
      "\n",
      "Processing ../../Dataset_Generation/Data/A_Multi.jsonl\n",
      "Average context tokens: 142.5\n",
      "Max context tokens: 209\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Dictionary to store token counts for each file\n",
    "token_counts = {}\n",
    "\n",
    "# Process each file\n",
    "for file_path, entries in file_data.items():\n",
    "    print(f\"\\nProcessing {file_path}\")\n",
    "    \n",
    "    # Lists to store token counts for this file\n",
    "    context_tokens = []\n",
    "    \n",
    "    # Process each entry\n",
    "    for entry in entries:\n",
    "        # Count tokens in context\n",
    "        context_token_count = len(tokenizer.encode(entry['context']))\n",
    "        context_tokens.append(context_token_count)\n",
    "        \n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_context = sum(context_tokens) / len(context_tokens)\n",
    "    max_context = max(context_tokens)\n",
    "    total_tokens = sum(context_tokens)  \n",
    "    \n",
    "    # Store results\n",
    "    token_counts[file_path] = {\n",
    "        'avg_context_tokens': avg_context,\n",
    "        'max_context_tokens': max_context,\n",
    "        'total_tokens': total_tokens\n",
    "    }\n",
    "    \n",
    "    print(f\"Average context tokens: {avg_context:.1f}\")\n",
    "    print(f\"Max context tokens: {max_context}\")\n",
    "    print(f\"Total tokens: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'../../Dataset_Generation/Data/F_Uni.jsonl': {'avg_context_tokens': 471.36466666666666,\n",
       "  'max_context_tokens': 579,\n",
       "  'total_tokens': 707047},\n",
       " '../../Dataset_Generation/Data/A_Uni.jsonl': {'avg_context_tokens': 553.3986666666667,\n",
       "  'max_context_tokens': 693,\n",
       "  'total_tokens': 830098},\n",
       " '../../Dataset_Generation/Data/T_Multi.jsonl': {'avg_context_tokens': 141.668,\n",
       "  'max_context_tokens': 201,\n",
       "  'total_tokens': 212502},\n",
       " '../../Dataset_Generation/Data/T_Uni.jsonl': {'avg_context_tokens': 479.484,\n",
       "  'max_context_tokens': 625,\n",
       "  'total_tokens': 719226},\n",
       " '../../Dataset_Generation/Data/F_Multi.jsonl': {'avg_context_tokens': 168.89133333333334,\n",
       "  'max_context_tokens': 230,\n",
       "  'total_tokens': 253337},\n",
       " '../../Dataset_Generation/Data/A_Multi.jsonl': {'avg_context_tokens': 142.5,\n",
       "  'max_context_tokens': 209,\n",
       "  'total_tokens': 213750}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

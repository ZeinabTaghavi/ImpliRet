{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.55, 0.528, 0.539)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Multi/gpt-4.1-2025-04-14/S_Multi_gpt-4.1-2025-04-14_-1_1747180230.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Uni/gpt-4.1-2025-04-14/S_Uni_gpt-4.1-2025-04-14_-1_1747176858.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7126666666666667, 0.572, 0.6423333333333333)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Multi/gpt-4.1-2025-04-14/S_Multi_gpt-4.1-2025-04-14_10_1747172694.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Uni/gpt-4.1-2025-04-14/S_Uni_gpt-4.1-2025-04-14_10_1747178716.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.988, 0.682, 0.835)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Multi/gpt-4.1-2025-04-14/S_Multi_gpt-4.1-2025-04-14_1_1747168253.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/S_Uni/gpt-4.1-2025-04-14/S_Uni_gpt-4.1-2025-04-14_1_1747170200.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11385555555555556, 0.152, 0.13292777777777778)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Multi/gpt-4.1-2025-04-14/A_Multi_gpt-4.1-2025-04-14_-1_1747211775.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Uni/gpt-4.1-2025-04-14/A_Uni_gpt-4.1-2025-04-14_-1_1747219381.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20287777777777777, 0.221, 0.2119388888888889)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Multi/gpt-4.1-2025-04-14/A_Multi_gpt-4.1-2025-04-14_10_1747211346.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Uni/gpt-4.1-2025-04-14/A_Uni_gpt-4.1-2025-04-14_10_1747218001.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9889111111111111, 0.994, 0.9914555555555555)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Multi/gpt-4.1-2025-04-14/A_Multi_gpt-4.1-2025-04-14_1_1747210873.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/A_Uni/gpt-4.1-2025-04-14/A_Uni_gpt-4.1-2025-04-14_1_1747216665.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15348888888888887, 0.032, 0.09274444444444443)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Multi/gpt-4.1-2025-04-14/T_Multi_gpt-4.1-2025-04-14_-1_1747175876.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Uni/gpt-4.1-2025-04-14/T_Uni_gpt-4.1-2025-04-14_-1_1747173955.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23256666666666667, 0.214, 0.22328333333333333)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Multi/gpt-4.1-2025-04-14/T_Multi_gpt-4.1-2025-04-14_10_1747176371.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Uni/gpt-4.1-2025-04-14/T_Uni_gpt-4.1-2025-04-14_10_1747161891.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9427444444444444, 0.948, 0.9453722222222222)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Multi/gpt-4.1-2025-04-14/T_Multi_gpt-4.1-2025-04-14_1_1747168216.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_1 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "import json\n",
    "file_path = \"Experiments/evaluation/results/T_Uni/gpt-4.1-2025-04-14/T_Uni_gpt-4.1-2025-04-14_1_1747159868.json\"\n",
    "# Load the JSON file\n",
    "with open('.'+file_path.split('Experiments/evaluation')[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "score_2 = sum([i['score']['rouge-1-recall'] for i in data['results']])/len(data['results'])\n",
    "score_2, score_1, (score_2+score_1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.38999999999999"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (83.50+99.14 +94.53)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

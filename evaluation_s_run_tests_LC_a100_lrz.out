Starting the job
INFO 06-09 11:43:33 [__init__.py:243] Automatically detected platform cuda.
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/F_Uni/llama_3.3_70b/F_Uni_llama_3.3_70b_-1_1749462216.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 06-09 11:43:37 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-09 11:43:37 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-09 11:43:37 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-09 11:43:53 [config.py:793] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.
INFO 06-09 11:43:53 [config.py:1875] Defaulting to use mp for distributed inference
INFO 06-09 11:43:53 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 06-09 11:43:55 [core.py:438] Waiting for init message from front-end.
INFO 06-09 11:43:55 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-09 11:43:55 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-09 11:43:55 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_19d89429'), local_subscribe_addr='ipc:///tmp/bf5b8737-0c92-4716-8709-6d2a6d22b8a8', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-09 11:43:58 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f120f81a420>
WARNING 06-09 11:43:58 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f120f69a420>
WARNING 06-09 11:43:58 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f10e7c3da60>
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:43:58 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ebc63acc'), local_subscribe_addr='ipc:///tmp/de0cb006-8c6f-4086-a323-519e25e836e9', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-09 11:43:58 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f120f852420>
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:43:58 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ffd34389'), local_subscribe_addr='ipc:///tmp/4166f332-8c79-45b9-ace4-c3fcf4368885', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:43:58 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2bad6809'), local_subscribe_addr='ipc:///tmp/56d71100-9cbd-454b-9e85-18b77844f3ae', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:43:58 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_021cfd9f'), local_subscribe_addr='ipc:///tmp/d7e04b6d-7373-4dec-9c7d-393a616816ef', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:05 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:05 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:05 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:05 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:05 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:05 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:05 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:05 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:17 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:17 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:17 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:17 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:17 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_04195590'), local_subscribe_addr='ipc:///tmp/ae00d0f9-0e38-4a67-b811-a2c09547a892', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:17 [parallel_state.py:1064] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:17 [parallel_state.py:1064] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:17 [parallel_state.py:1064] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:17 [parallel_state.py:1064] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m WARNING 06-09 11:44:17 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m WARNING 06-09 11:44:17 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m WARNING 06-09 11:44:17 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m WARNING 06-09 11:44:17 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:17 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:17 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:17 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:17 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:18 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:18 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:18 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:18 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:18 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:18 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:18 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:18 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:44:19 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:44:19 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:44:19 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:44:19 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:47:18 [default_loader.py:280] Loading weights took 177.10 seconds
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:47:18 [default_loader.py:280] Loading weights took 178.11 seconds
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:47:18 [default_loader.py:280] Loading weights took 177.37 seconds
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:47:18 [default_loader.py:280] Loading weights took 177.80 seconds
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:47:18 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 180.408147 seconds
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:47:18 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 180.458981 seconds
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:47:18 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 180.432515 seconds
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:47:18 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 180.403061 seconds
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:47:42 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:47:42 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:47:42 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:47:42 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:47:42 [backends.py:469] Dynamo bytecode transform time: 24.10 s
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:47:42 [backends.py:469] Dynamo bytecode transform time: 24.10 s
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:47:42 [backends.py:469] Dynamo bytecode transform time: 24.10 s
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:47:42 [backends.py:469] Dynamo bytecode transform time: 24.11 s
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:47:57 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 12.889 s
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:47:57 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 12.988 s
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:47:57 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 13.446 s
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:48:01 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 17.309 s
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:48:04 [monitor.py:33] torch.compile takes 24.10 s in total
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:48:04 [monitor.py:33] torch.compile takes 24.11 s in total
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:48:04 [monitor.py:33] torch.compile takes 24.10 s in total
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:48:04 [monitor.py:33] torch.compile takes 24.10 s in total
INFO 06-09 11:48:15 [kv_cache_utils.py:637] GPU KV cache size: 566,400 tokens
INFO 06-09 11:48:15 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
INFO 06-09 11:48:15 [kv_cache_utils.py:637] GPU KV cache size: 563,744 tokens
INFO 06-09 11:48:15 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 11:48:15 [kv_cache_utils.py:637] GPU KV cache size: 563,744 tokens
INFO 06-09 11:48:15 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 11:48:15 [kv_cache_utils.py:637] GPU KV cache size: 566,400 tokens
INFO 06-09 11:48:15 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:48:53 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:48:54 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:49:01 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:49:01 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=3835662)[0;0m INFO 06-09 11:49:02 [gpu_model_runner.py:1933] Graph capturing finished in 48 secs, took 4.77 GiB
[1;36m(VllmWorker rank=2 pid=3835661)[0;0m INFO 06-09 11:49:03 [gpu_model_runner.py:1933] Graph capturing finished in 48 secs, took 4.77 GiB
[1;36m(VllmWorker rank=0 pid=3835659)[0;0m INFO 06-09 11:49:03 [gpu_model_runner.py:1933] Graph capturing finished in 48 secs, took 4.77 GiB
[1;36m(VllmWorker rank=1 pid=3835660)[0;0m INFO 06-09 11:49:03 [gpu_model_runner.py:1933] Graph capturing finished in 48 secs, took 4.77 GiB
INFO 06-09 11:49:03 [core.py:167] init engine (profile, create kv cache, warmup model) took 104.52 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 06-09 11:49:08 [chat_utils.py:419] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/F_Uni/llama_3.3_70b/F_Uni_llama_3.3_70b_-1_1749462216.json
[run_tests] Experiment completed.
INFO 06-09 12:20:53 [__init__.py:243] Automatically detected platform cuda.
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/F_Uni/llama_3.3_70b/F_Uni_llama_3.3_70b_1_1749464457.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 06-09 12:20:57 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-09 12:20:57 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-09 12:20:57 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-09 12:21:12 [config.py:793] This model supports multiple tasks: {'generate', 'score', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 06-09 12:21:12 [config.py:1875] Defaulting to use mp for distributed inference
INFO 06-09 12:21:12 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 06-09 12:21:15 [core.py:438] Waiting for init message from front-end.
INFO 06-09 12:21:15 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-09 12:21:15 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-09 12:21:15 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_6fffb639'), local_subscribe_addr='ipc:///tmp/d938b4a0-adbd-4ce6-86a1-7fc74bbe90ee', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-09 12:21:15 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f34cb0f60f0>
WARNING 06-09 12:21:15 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f34cb0f7890>
WARNING 06-09 12:21:15 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f34caffff50>
WARNING 06-09 12:21:15 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f34cb0f7890>
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:15 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_416cc0aa'), local_subscribe_addr='ipc:///tmp/7c5e00c3-4dfe-44b6-8b4a-5accc9f22753', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:15 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1a18323f'), local_subscribe_addr='ipc:///tmp/6f3d5e28-8915-4107-a9b3-59828ccc7435', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:15 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_99111e5a'), local_subscribe_addr='ipc:///tmp/4794a40f-c0c0-4584-bee2-400eb5ad5fae', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:15 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_57730282'), local_subscribe_addr='ipc:///tmp/cebee9ee-9975-4910-8664-75563fd3e981', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:22 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:22 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:22 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:22 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:22 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:22 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:22 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:22 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:34 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:34 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:34 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:34 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:34 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_6aeeb875'), local_subscribe_addr='ipc:///tmp/6fb379ad-f78e-4d20-b2dc-cf80c3344444', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:34 [parallel_state.py:1064] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:34 [parallel_state.py:1064] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:34 [parallel_state.py:1064] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:34 [parallel_state.py:1064] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m WARNING 06-09 12:21:34 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m WARNING 06-09 12:21:34 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m WARNING 06-09 12:21:34 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m WARNING 06-09 12:21:34 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:34 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:34 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:34 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:34 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:35 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:35 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:35 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:35 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:35 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:35 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:35 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:35 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:21:35 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:21:35 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:21:35 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:21:36 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:24:49 [default_loader.py:280] Loading weights took 191.83 seconds
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:24:49 [default_loader.py:280] Loading weights took 193.63 seconds
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:24:49 [default_loader.py:280] Loading weights took 192.73 seconds
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:24:49 [default_loader.py:280] Loading weights took 192.32 seconds
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:24:50 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 194.937699 seconds
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:24:50 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 194.953225 seconds
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:24:50 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 195.007154 seconds
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:24:50 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 195.024119 seconds
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:25:07 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:25:07 [backends.py:469] Dynamo bytecode transform time: 17.59 s
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:25:07 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:25:07 [backends.py:469] Dynamo bytecode transform time: 17.60 s
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:25:13 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:25:13 [backends.py:469] Dynamo bytecode transform time: 23.05 s
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:25:13 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:25:13 [backends.py:469] Dynamo bytecode transform time: 23.09 s
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:25:21 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 12.583 s
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:25:22 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 13.051 s
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:25:27 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 12.239 s
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:25:33 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 18.223 s
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:25:37 [monitor.py:33] torch.compile takes 23.05 s in total
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:25:37 [monitor.py:33] torch.compile takes 17.60 s in total
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:25:37 [monitor.py:33] torch.compile takes 23.09 s in total
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:25:37 [monitor.py:33] torch.compile takes 17.59 s in total
INFO 06-09 12:25:48 [kv_cache_utils.py:637] GPU KV cache size: 566,400 tokens
INFO 06-09 12:25:48 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
INFO 06-09 12:25:48 [kv_cache_utils.py:637] GPU KV cache size: 563,744 tokens
INFO 06-09 12:25:48 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 12:25:48 [kv_cache_utils.py:637] GPU KV cache size: 563,744 tokens
INFO 06-09 12:25:48 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 12:25:48 [kv_cache_utils.py:637] GPU KV cache size: 566,400 tokens
INFO 06-09 12:25:48 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:26:26 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:26:30 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:26:30 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:26:33 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=3844580)[0;0m INFO 06-09 12:26:35 [gpu_model_runner.py:1933] Graph capturing finished in 47 secs, took 4.77 GiB
[1;36m(VllmWorker rank=0 pid=3844577)[0;0m INFO 06-09 12:26:35 [gpu_model_runner.py:1933] Graph capturing finished in 47 secs, took 4.77 GiB
[1;36m(VllmWorker rank=2 pid=3844579)[0;0m INFO 06-09 12:26:35 [gpu_model_runner.py:1933] Graph capturing finished in 47 secs, took 4.77 GiB
[1;36m(VllmWorker rank=1 pid=3844578)[0;0m INFO 06-09 12:26:35 [gpu_model_runner.py:1933] Graph capturing finished in 47 secs, took 4.77 GiB
INFO 06-09 12:26:35 [core.py:167] init engine (profile, create kv cache, warmup model) took 105.20 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 06-09 12:26:40 [chat_utils.py:419] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/F_Uni/llama_3.3_70b/F_Uni_llama_3.3_70b_1_1749464457.json
[run_tests] Experiment completed.
INFO 06-09 12:28:12 [__init__.py:243] Automatically detected platform cuda.
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/F_Uni/llama_3.3_70b/F_Uni_llama_3.3_70b_10_1749464896.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 06-09 12:28:16 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-09 12:28:16 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-09 12:28:16 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-09 12:28:31 [config.py:793] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 06-09 12:28:31 [config.py:1875] Defaulting to use mp for distributed inference
INFO 06-09 12:28:31 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 06-09 12:28:34 [core.py:438] Waiting for init message from front-end.
INFO 06-09 12:28:34 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-09 12:28:34 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-09 12:28:34 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_83226d55'), local_subscribe_addr='ipc:///tmp/86c69aaf-beb4-4b66-8406-0e8ec27eb480', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-09 12:28:34 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8ce97658e0>
WARNING 06-09 12:28:34 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8ce9765460>
WARNING 06-09 12:28:34 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8e11342420>
WARNING 06-09 12:28:34 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8e111a2420>
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:34 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0f95fbb1'), local_subscribe_addr='ipc:///tmp/21f5f947-42f1-4ae9-a0c5-f2e9224f1a75', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:34 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fd878f05'), local_subscribe_addr='ipc:///tmp/080e03de-84c9-453d-b6ab-c8b81a5b1e1e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:34 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_01a25fd8'), local_subscribe_addr='ipc:///tmp/692b41b6-11e1-40ef-a47e-4d3fa0a3c6d1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:34 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ad04a7aa'), local_subscribe_addr='ipc:///tmp/dd0cd178-068b-448b-848c-06cc13e3c18f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:41 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:41 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:41 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:41 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:41 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:41 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:41 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:41 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:53 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:53 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:53 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:53 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:53 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_119fb83f'), local_subscribe_addr='ipc:///tmp/b210cab8-3f33-4e5c-82a0-08ca87bf3d28', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:53 [parallel_state.py:1064] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:53 [parallel_state.py:1064] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:53 [parallel_state.py:1064] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:53 [parallel_state.py:1064] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m WARNING 06-09 12:28:53 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m WARNING 06-09 12:28:53 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m WARNING 06-09 12:28:53 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m WARNING 06-09 12:28:53 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:54 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:54 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:54 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:54 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:54 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:54 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:54 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:54 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:54 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:54 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:54 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:54 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:28:54 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:28:55 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:28:55 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:28:55 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:32:11 [default_loader.py:280] Loading weights took 196.19 seconds
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:32:11 [default_loader.py:280] Loading weights took 196.55 seconds
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:32:11 [default_loader.py:280] Loading weights took 195.50 seconds
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:32:11 [default_loader.py:280] Loading weights took 195.93 seconds
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:32:12 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 197.795517 seconds
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:32:12 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 197.792818 seconds
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:32:12 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 197.829600 seconds
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:32:12 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 197.886086 seconds
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:32:30 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:32:30 [backends.py:469] Dynamo bytecode transform time: 17.68 s
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:32:30 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:32:30 [backends.py:469] Dynamo bytecode transform time: 17.82 s
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:32:35 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:32:35 [backends.py:469] Dynamo bytecode transform time: 23.00 s
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:32:36 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:32:36 [backends.py:469] Dynamo bytecode transform time: 24.20 s
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:32:44 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 12.312 s
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:32:49 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 12.160 s
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:32:49 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 17.800 s
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:32:56 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 18.194 s
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:33:00 [monitor.py:33] torch.compile takes 24.20 s in total
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:33:00 [monitor.py:33] torch.compile takes 23.00 s in total
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:33:00 [monitor.py:33] torch.compile takes 17.82 s in total
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:33:00 [monitor.py:33] torch.compile takes 17.68 s in total
INFO 06-09 12:33:11 [kv_cache_utils.py:637] GPU KV cache size: 566,400 tokens
INFO 06-09 12:33:11 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
INFO 06-09 12:33:11 [kv_cache_utils.py:637] GPU KV cache size: 563,744 tokens
INFO 06-09 12:33:11 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 12:33:11 [kv_cache_utils.py:637] GPU KV cache size: 563,744 tokens
INFO 06-09 12:33:11 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 12:33:11 [kv_cache_utils.py:637] GPU KV cache size: 566,400 tokens
INFO 06-09 12:33:11 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:33:51 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:33:51 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:33:52 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:33:55 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=3846807)[0;0m INFO 06-09 12:33:57 [gpu_model_runner.py:1933] Graph capturing finished in 46 secs, took 4.77 GiB
[1;36m(VllmWorker rank=3 pid=3846808)[0;0m INFO 06-09 12:33:57 [gpu_model_runner.py:1933] Graph capturing finished in 46 secs, took 4.77 GiB
[1;36m(VllmWorker rank=1 pid=3846806)[0;0m INFO 06-09 12:33:57 [gpu_model_runner.py:1933] Graph capturing finished in 46 secs, took 4.77 GiB
[1;36m(VllmWorker rank=0 pid=3846805)[0;0m INFO 06-09 12:33:57 [gpu_model_runner.py:1933] Graph capturing finished in 46 secs, took 4.77 GiB
INFO 06-09 12:33:57 [core.py:167] init engine (profile, create kv cache, warmup model) took 105.35 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 06-09 12:34:02 [chat_utils.py:419] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

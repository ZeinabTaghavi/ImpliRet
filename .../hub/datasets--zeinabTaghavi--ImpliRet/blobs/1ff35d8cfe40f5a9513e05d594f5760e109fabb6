---
pretty_name: ImpliRet
license:
  - mit
language:
  - en

configs:
  - config_name: multispeaker
    description: "Multi-speaker forum style. Reasoning Categories: Arithmetic (arithmetic), World-Knowledge (wknow), Temporal (temporal)"
    data_files:
      - split: arithmetic
        path: "data/A_Multi.csv"
      - split: wknow
        path: "data/W_Multi.csv"
      - split: temporal
        path: "data/T_Multi.csv"

  - config_name: unispeaker
    description: "Uni-speaker, chat style datasets. Reasoning Categories: Arithmetic (arithmetic), World-Knowledge (wknow), Temporal (temporal)"
    data_files:
      - split: arithmetic
        path: "data/A_Uni.csv"
      - split: wknow
        path: "data/W_Uni.csv"
      - split: temporal
        path: "data/T_Uni.csv"

viewer: true
---

---

# 📚 ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge

[![arXiv](https://img.shields.io/badge/arXiv-2506.14407-b31b1b.svg?logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.14407)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-181717?logo=github&logoColor=white)](https://github.com/ZeinabTaghavi/IMPLIRET)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ZeinabTaghavi/ImpliRet/blob/master/notebook.ipynb)

**ImpliRet** (**Impli**cit Fact **Ret**rieval) is a six-subset benchmark that shifts the reasoning burden from
the *query* to the *documents themselves*.  
A query is always simple (e.g. *"Who visited a museum on October 06 2024?"*) but the relevant document
contains the answer **only implicitly**—via:

| Reasoning category |    split   | Implicit cue inside the document | Query asks for … |
|--------------------|------------|----------------------------------|------------------|
| **Arithmetic**     |`arithmetic`|relative price or percentage (*"model 2019: 2.5 × more expensive…"*) | absolute price |
| **Temporal**       |`temporal`  | relative date (*"seven days ago"*) | explicit date |
| **World Knowledge**|`wknow`     | landmark that implies a country (*"Big Ben" ⇒ UK*) | the person / item in that country |

For each category we generate two discourse styles:

|      Discourse   |      name     | style   |
|------------------|---------------|---------|
| **Multi Speaker**|`multispeaker` | Multi-speaker **forum thread** (one post ↔︎ one response)
| **Uni Speaker**  |`unispeaker`   | Uni-speaker **chat** (one-chat ↔︎ 10-turn dialogue)   

Every CSV entry contains the positive **document**, the **query**, its **answer**, and auxiliary
metadata (`id`, `tuple_set_id`, … see *Field specification* below).

<p align="center">
  <img src="Figure.png" alt="Figure 1 – Example instance in ImpliRet" width="60%">
</p>

*Figure 1. Example of an implicit‑fact retrieval instance in ImpliRet. As you can see the positive passage gets lower score.*

More details are in the [paper](https://arxiv.org/abs/2506.14407).
---

## ✨ Key features

* **Document-side reasoning:** retrievers cannot rely on query expansion or multi-hop tricks— they must
  understand the *document*.
* **Three reasoning types × two styles:** exposes models to arithmetic, temporal and world-knowledge
  inference in both conversational and forum prose.
* **Negatives in-pool:** each query's pool contains 29 stylistically similar distractors.
---

## 🔖 Field specification

| Field | Type | Present in | Description |
|-------|------|------------|-------------|
| `id`            | `string` | all | Unique example id |
| `tuple_set_id`  | `string` | all | Id of the 30-tuple pool |
| `forum_question`| `string` | multispeaker | Starting question for the forum thread with the same `tuple_set_id` |
| `main_speaker`  | `string` | unispeaker   | Main speaker name in the dialogue whith the same `tuple_set_id` |
| `pos_document`  | `string` | all | The only relevant document |
| `question`      | `string` | all | Retrieval query |
| `answer`        | `string` | all | Gold answer string |
| `explicit_hint` | `dict`   | all | Resolved explicit fact(s) (e.g. `{message_date, offset_days}`) splited by special character `**` |

---

## 📊 Dataset statistics

| Category | Style | Docs / Queries | Avg. tokens | Total tokens |
|----------|-------|---------------:|------------:|-------------:|
| Arithmetic | Uni-speaker | 1500 | 553 | 830 k |
| Arithmetic | Multi-speaker | 1500 | 142 | 214 k |
| World Knowledge | Uni-speaker | 1500 | 471 | 707 k |
| World Knowledge | Multi-speaker | 1500 | 168 | 253 k |
| Temporal | Uni-speaker | 1500 | 479 | 719 k |
| Temporal | Multi-speaker | 1500 | 141 | 213 k |

*(token counts computed with the GPT-2 tokenizer)*

---
## 📈 Results

### 🔬 Retrieval Evaluatio

The table below reports **nDCG@10** (↑ higher is better) for our baseline retrievers.

| Retriever | W. Know. | Arithmetic | Temporal | Average |
|-----------|:-------:|:----------:|:--------:|:-------:|
| ***Sparse*** |  |  |  |  |
| BM25 | 14.10 | 11.06 | 11.22 | 12.13 |
| ***Late Interaction*** |  |  |  |  |
| ColBERT v2 | 16.04 | 14.93 | 12.20 | 14.39 |
| ***Dense Encoders*** |  |  |  |  |
| Contriever | 16.15 | 13.84 | 12.59 | 14.19 |
| Dragon+ | 17.15 | **14.61** | 12.53 | 14.76 |
| ReasonIR‑8B | **19.53** | 10.74 | **14.93** | **15.07** |
| ***Knowledge‑Graph‑Augmented*** |  |  |  |  |
| HippoRAG 2 | 16.38 | 14.21 | 12.50 | 14.36 |

*Table 2. nDCG@10 retrieval performance averaged over uni‑speaker and multi‑speaker documents.*

### 🧩 RAG‑style Evaluation

The table below shows **ROUGE‑1 recall** (R‑1@k) for two long‑context LLM readers when the top‑*k* retrieved documents (oracle setting) are supplied.

| Experiment | *k* | W. Know. | Arithmetic | Temporal | Average |
|------------|:---:|:--------:|:----------:|:--------:|:-------:|
| **Llama 3.3 70B** | 1   | **73.79** | **90.13** | **81.85** | **81.92** |
|                | 10  | 27.37 | 16.98 | 25.23 | 23.19 |
|                | All | 17.43 | 4.42  | 10.29 | 10.71 |
| **GPT‑4.1**     | 1   | **93.24** | **92.12** | **84.90** | **88.05** |
|                | 10  | 62.21 | 23.86 | 15.59 | 35.06 |
|                | All | 53.91 | 9.28  |  6.93 | 22.90 |

*Table 3. ROUGE‑1 recall (R‑1@k), averaged over uni‑speaker and multi‑speaker documents.*

---

## 🗄️ Loading example

```python
from datasets import load_dataset

# forum style, temporal reasoning
ds = load_dataset("zeinabTaghavi/ImpliRet",
                  name="multispeaker",
                  split="temporal")

print(ds[0]["question"])
print(ds[0]["pos_document"])
```

---
## 📜 Citation

```bibtex
@misc{taghavi2025impliret,
  author={Zeinab Sadat Taghavi and Ali Modarressi and Yunpu Ma and Hinrich Sch{\"u}tze},
  keywords = {Computation and Language (cs.CL),  Artificial Intelligence (cs.AI),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge},
  year = {2025},
}
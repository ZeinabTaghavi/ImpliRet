Starting the job
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_-1_1747312620.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 05-15 14:37:17 config.py:510] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-15 14:37:17 config.py:1310] Defaulting to use mp for distributed inference
WARNING 05-15 14:37:17 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 05-15 14:37:17 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 05-15 14:37:17 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 05-15 14:37:18 multiproc_worker_utils.py:312] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 05-15 14:37:18 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 05-15 14:37:20 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:37:21 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:37:21 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:37:21 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:37:21 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:37:21 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:37:21 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 05-15 14:37:27 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:37:27 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:37:27 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:37:27 utils.py:918] Found nccl from library libnccl.so.2
INFO 05-15 14:37:27 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:37:27 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:37:27 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:37:27 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 14:37:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:37:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:37:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:37:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 05-15 14:37:40 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_427639e0'), local_subscribe_port=48543, remote_subscribe_port=None)
INFO 05-15 14:37:40 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:37:40 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:37:40 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:37:40 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 05-15 14:37:42 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:37:42 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:37:42 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:37:42 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:41:35 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:41:35 model_runner.py:1099] Loading model weights took 32.8892 GB
INFO 05-15 14:41:35 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:41:35 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:42:03 worker.py:241] Memory profiling takes 27.01 seconds
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:42:03 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:42:03 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.48GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 48.11GiB.
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:42:03 worker.py:241] Memory profiling takes 27.05 seconds
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:42:03 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:42:03 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:42:03 worker.py:241] Memory profiling takes 27.09 seconds
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:42:03 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:42:03 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 48.31GiB.
INFO 05-15 14:42:03 worker.py:241] Memory profiling takes 27.41 seconds
INFO 05-15 14:42:03 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-15 14:42:03 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.50GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 47.10GiB.
INFO 05-15 14:42:03 distributed_gpu_executor.py:57] # GPU blocks: 38580, # CPU blocks: 3276
INFO 05-15 14:42:03 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 4.71x
INFO 05-15 14:42:13 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:42:13 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:42:13 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:42:13 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-15 14:42:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:42:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:42:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:42:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 14:42:39 model_runner.py:1535] Graph capturing finished in 27 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 14:42:39 model_runner.py:1535] Graph capturing finished in 27 secs, took 2.33 GiB
INFO 05-15 14:42:39 model_runner.py:1535] Graph capturing finished in 27 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 14:42:39 model_runner.py:1535] Graph capturing finished in 27 secs, took 2.33 GiB
INFO 05-15 14:42:39 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 63.86 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 05-15 14:42:41 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_-1_1747312620.json
[run_tests] Experiment completed.
INFO 05-15 15:13:42 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2086075)[0;0m INFO 05-15 15:13:42 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2086076)[0;0m INFO 05-15 15:13:42 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2086077)[0;0m INFO 05-15 15:13:42 multiproc_worker_utils.py:247] Worker exiting
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_1_1747314856.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 05-15 15:14:28 config.py:510] This model supports multiple tasks: {'score', 'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-15 15:14:28 config.py:1310] Defaulting to use mp for distributed inference
WARNING 05-15 15:14:28 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 05-15 15:14:28 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 05-15 15:14:28 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 05-15 15:14:29 multiproc_worker_utils.py:312] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 05-15 15:14:29 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 05-15 15:14:31 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:14:32 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:14:32 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:14:32 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:14:32 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:14:32 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:14:32 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 05-15 15:14:38 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:14:38 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:14:38 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:14:38 utils.py:918] Found nccl from library libnccl.so.2
INFO 05-15 15:14:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:14:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:14:38 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:14:38 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 15:14:50 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:14:50 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:14:50 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:14:50 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 05-15 15:14:51 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0f39d15c'), local_subscribe_port=34547, remote_subscribe_port=None)
INFO 05-15 15:14:51 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:14:51 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:14:51 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:14:51 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 05-15 15:14:52 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:14:52 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:14:52 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:14:53 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:19:14 model_runner.py:1099] Loading model weights took 32.8892 GB
INFO 05-15 15:19:14 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:19:14 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:19:14 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:19:38 worker.py:241] Memory profiling takes 23.53 seconds
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:19:38 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:19:38 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.07GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.54GiB.
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:19:38 worker.py:241] Memory profiling takes 23.53 seconds
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:19:38 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:19:38 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:19:38 worker.py:241] Memory profiling takes 23.55 seconds
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:19:38 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:19:38 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
INFO 05-15 15:19:38 worker.py:241] Memory profiling takes 24.18 seconds
INFO 05-15 15:19:38 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-15 15:19:38 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.50GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 47.10GiB.
INFO 05-15 15:19:39 distributed_gpu_executor.py:57] # GPU blocks: 38580, # CPU blocks: 3276
INFO 05-15 15:19:39 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 4.71x
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:19:48 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:19:48 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:19:48 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-15 15:19:48 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-15 15:20:13 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:20:13 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:20:13 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:20:13 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:20:14 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:20:14 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 15:20:14 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:20:14 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 15:20:14 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 60.29 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 05-15 15:20:16 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_1_1747314856.json
[run_tests] Experiment completed.
INFO 05-15 15:21:54 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2095394)[0;0m INFO 05-15 15:21:54 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2095396)[0;0m INFO 05-15 15:21:54 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2095395)[0;0m INFO 05-15 15:21:54 multiproc_worker_utils.py:247] Worker exiting
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_10_1747315338.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 05-15 15:22:31 config.py:510] This model supports multiple tasks: {'embed', 'reward', 'score', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 05-15 15:22:31 config.py:1310] Defaulting to use mp for distributed inference
WARNING 05-15 15:22:31 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 05-15 15:22:31 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 05-15 15:22:31 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 05-15 15:22:32 multiproc_worker_utils.py:312] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 05-15 15:22:32 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 05-15 15:22:34 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:22:35 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:22:35 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:22:35 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:22:35 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:22:35 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:22:35 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:22:40 utils.py:918] Found nccl from library libnccl.so.2
INFO 05-15 15:22:40 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:22:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:22:40 utils.py:918] Found nccl from library libnccl.so.2
INFO 05-15 15:22:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:22:40 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:22:40 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:22:40 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 15:22:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:22:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:22:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:22:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 05-15 15:22:53 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_656ab064'), local_subscribe_port=33501, remote_subscribe_port=None)
INFO 05-15 15:22:53 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:22:53 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:22:53 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:22:53 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:22:54 weight_utils.py:251] Using model weights format ['*.safetensors']
INFO 05-15 15:22:54 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:22:54 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:22:54 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:27:00 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:27:00 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:27:00 model_runner.py:1099] Loading model weights took 32.8892 GB
INFO 05-15 15:27:00 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:27:16 worker.py:241] Memory profiling takes 16.55 seconds
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:27:16 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:27:16 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:27:16 worker.py:241] Memory profiling takes 16.56 seconds
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:27:16 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:27:16 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.07GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.54GiB.
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:27:16 worker.py:241] Memory profiling takes 16.58 seconds
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:27:16 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:27:16 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
INFO 05-15 15:27:16 worker.py:241] Memory profiling takes 16.60 seconds
INFO 05-15 15:27:16 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-15 15:27:16 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.50GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 47.10GiB.
INFO 05-15 15:27:17 distributed_gpu_executor.py:57] # GPU blocks: 38580, # CPU blocks: 3276
INFO 05-15 15:27:17 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 4.71x
INFO 05-15 15:27:26 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:27:26 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:27:26 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:27:26 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:27:51 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:27:51 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:27:51 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
INFO 05-15 15:27:51 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
INFO 05-15 15:27:52 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:27:52 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:27:52 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:27:52 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 15:27:52 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 52.69 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 05-15 15:27:54 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_10_1747315338.json
[run_tests] Experiment completed.
INFO 05-15 15:38:12 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2097695)[0;0m INFO 05-15 15:38:12 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2097694)[0;0m INFO 05-15 15:38:12 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2097696)[0;0m INFO 05-15 15:38:12 multiproc_worker_utils.py:247] Worker exiting
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Multi/llama_3.3_70b/A_Multi_llama_3.3_70b_-1_1747316318.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 05-15 15:38:50 config.py:510] This model supports multiple tasks: {'generate', 'reward', 'embed', 'score', 'classify'}. Defaulting to 'generate'.
INFO 05-15 15:38:50 config.py:1310] Defaulting to use mp for distributed inference
WARNING 05-15 15:38:50 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 05-15 15:38:50 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 05-15 15:38:50 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 05-15 15:38:51 multiproc_worker_utils.py:312] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 05-15 15:38:51 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 05-15 15:38:53 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:38:53 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:38:53 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:38:53 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:38:53 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:38:53 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:38:53 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:38:59 utils.py:918] Found nccl from library libnccl.so.2
INFO 05-15 15:38:59 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:38:59 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:38:59 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:38:59 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 15:38:59 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:38:59 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:38:59 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 15:39:11 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:39:11 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:39:11 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:39:11 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 05-15 15:39:12 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d1ce62a6'), local_subscribe_port=60571, remote_subscribe_port=None)
INFO 05-15 15:39:12 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:39:12 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:39:12 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:39:12 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 05-15 15:39:13 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:39:13 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:39:13 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:39:14 weight_utils.py:251] Using model weights format ['*.safetensors']
INFO 05-15 15:43:32 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:43:32 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:43:32 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:43:32 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:43:52 worker.py:241] Memory profiling takes 20.40 seconds
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:43:52 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:43:52 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:43:52 worker.py:241] Memory profiling takes 20.40 seconds
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:43:52 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:43:52 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.07GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.54GiB.
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:43:52 worker.py:241] Memory profiling takes 20.41 seconds
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:43:52 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:43:52 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
INFO 05-15 15:43:53 worker.py:241] Memory profiling takes 20.69 seconds
INFO 05-15 15:43:53 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-15 15:43:53 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.50GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 47.10GiB.
INFO 05-15 15:43:53 distributed_gpu_executor.py:57] # GPU blocks: 38580, # CPU blocks: 3276
INFO 05-15 15:43:53 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 4.71x
INFO 05-15 15:44:03 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:44:03 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:44:03 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:44:03 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-15 15:44:28 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:44:28 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:44:28 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:44:28 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:44:29 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:44:29 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:44:29 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 15:44:29 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 15:44:29 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 57.01 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 05-15 15:44:30 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/A_Multi/llama_3.3_70b/A_Multi_llama_3.3_70b_-1_1747316318.json
[run_tests] Experiment completed.
INFO 05-15 15:47:17 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2101943)[0;0m INFO 05-15 15:47:17 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2101944)[0;0m INFO 05-15 15:47:17 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2101945)[0;0m INFO 05-15 15:47:17 multiproc_worker_utils.py:247] Worker exiting
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Multi/llama_3.3_70b/A_Multi_llama_3.3_70b_1_1747316861.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 05-15 15:47:52 config.py:510] This model supports multiple tasks: {'score', 'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 05-15 15:47:52 config.py:1310] Defaulting to use mp for distributed inference
WARNING 05-15 15:47:52 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 05-15 15:47:52 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 05-15 15:47:52 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 05-15 15:47:53 multiproc_worker_utils.py:312] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 05-15 15:47:54 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 05-15 15:47:56 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:47:56 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:47:56 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:47:56 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:47:56 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:47:56 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:47:56 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:48:02 utils.py:918] Found nccl from library libnccl.so.2
INFO 05-15 15:48:02 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:48:02 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:48:02 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:48:02 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 15:48:02 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:48:02 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:48:02 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 15:48:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:48:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:48:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:48:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 05-15 15:48:15 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_fe80e890'), local_subscribe_port=39225, remote_subscribe_port=None)
INFO 05-15 15:48:15 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:48:15 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:48:15 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:48:15 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 05-15 15:48:16 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:48:16 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:48:16 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:48:16 weight_utils.py:251] Using model weights format ['*.safetensors']
INFO 05-15 15:52:42 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:52:42 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:52:42 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:52:42 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:52:59 worker.py:241] Memory profiling takes 16.53 seconds
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:52:59 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:52:59 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.07GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.54GiB.
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:52:59 worker.py:241] Memory profiling takes 16.53 seconds
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:52:59 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:52:59 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:52:59 worker.py:241] Memory profiling takes 16.54 seconds
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:52:59 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:52:59 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
INFO 05-15 15:52:59 worker.py:241] Memory profiling takes 16.78 seconds
INFO 05-15 15:52:59 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-15 15:52:59 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.50GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 47.10GiB.
INFO 05-15 15:53:00 distributed_gpu_executor.py:57] # GPU blocks: 38580, # CPU blocks: 3276
INFO 05-15 15:53:00 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 4.71x
INFO 05-15 15:53:09 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:53:09 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:53:09 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:53:09 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:53:34 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:53:34 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
INFO 05-15 15:53:34 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:53:34 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:53:35 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:53:35 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 15:53:35 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:53:35 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 15:53:35 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 52.60 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 05-15 15:53:36 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/A_Multi/llama_3.3_70b/A_Multi_llama_3.3_70b_1_1747316861.json
[run_tests] Experiment completed.
INFO 05-15 15:54:07 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2104552)[0;0m INFO 05-15 15:54:07 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2104554)[0;0m INFO 05-15 15:54:07 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2104553)[0;0m INFO 05-15 15:54:07 multiproc_worker_utils.py:247] Worker exiting
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Multi/llama_3.3_70b/A_Multi_llama_3.3_70b_10_1747317268.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 05-15 15:54:41 config.py:510] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.
INFO 05-15 15:54:41 config.py:1310] Defaulting to use mp for distributed inference
WARNING 05-15 15:54:41 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 05-15 15:54:41 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 05-15 15:54:41 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 05-15 15:54:42 multiproc_worker_utils.py:312] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 05-15 15:54:42 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 05-15 15:54:43 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:54:44 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:54:44 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:54:44 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:54:44 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:54:44 selector.py:120] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:54:44 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
INFO 05-15 15:54:50 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:54:50 utils.py:918] Found nccl from library libnccl.so.2
INFO 05-15 15:54:50 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:54:50 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:54:50 utils.py:918] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:54:50 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:54:50 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:54:50 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 05-15 15:55:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:55:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:55:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:55:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 05-15 15:55:02 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_dd2daace'), local_subscribe_port=47041, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:55:02 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 05-15 15:55:02 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:55:02 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:55:02 model_runner.py:1094] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 05-15 15:55:03 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:55:03 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:55:03 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:55:03 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:59:27 model_runner.py:1099] Loading model weights took 32.8892 GB
INFO 05-15 15:59:27 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:59:27 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:59:27 model_runner.py:1099] Loading model weights took 32.8892 GB
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:59:42 worker.py:241] Memory profiling takes 15.92 seconds
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:59:42 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:59:42 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:59:43 worker.py:241] Memory profiling takes 15.95 seconds
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:59:43 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:59:43 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.27GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.34GiB.
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:59:43 worker.py:241] Memory profiling takes 15.96 seconds
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:59:43 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:59:43 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.07GiB; PyTorch activation peak memory takes 0.21GiB; the rest of the memory reserved for KV Cache is 48.54GiB.
INFO 05-15 15:59:43 worker.py:241] Memory profiling takes 16.10 seconds
INFO 05-15 15:59:43 worker.py:241] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-15 15:59:43 worker.py:241] model weights take 32.89GiB; non_torch_memory takes 2.50GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 47.10GiB.
INFO 05-15 15:59:43 distributed_gpu_executor.py:57] # GPU blocks: 38580, # CPU blocks: 3276
INFO 05-15 15:59:43 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 4.71x
INFO 05-15 15:59:52 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 15:59:52 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 15:59:52 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 15:59:52 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 16:00:17 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
INFO 05-15 16:00:17 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 16:00:17 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 16:00:17 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
INFO 05-15 16:00:18 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 16:00:18 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 16:00:18 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 16:00:18 model_runner.py:1535] Graph capturing finished in 26 secs, took 2.33 GiB
INFO 05-15 16:00:18 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 51.93 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 05-15 16:00:20 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.

 ----------- [STEP 4] Processing Results -----------

 ----------- [STEP 6] Computing Scores -----------

 ----------- [STEP 7] Saving Results -----------

 ----------- Evaluation Complete -----------
Results saved to: ./Experiments/evaluation/results/A_Multi/llama_3.3_70b/A_Multi_llama_3.3_70b_10_1747317268.json
[run_tests] Experiment completed.
INFO 05-15 16:01:54 multiproc_worker_utils.py:140] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=2106453)[0;0m INFO 05-15 16:01:54 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2106452)[0;0m INFO 05-15 16:01:54 multiproc_worker_utils.py:247] Worker exiting
[1;36m(VllmWorkerProcess pid=2106454)[0;0m INFO 05-15 16:01:54 multiproc_worker_utils.py:247] Worker exiting

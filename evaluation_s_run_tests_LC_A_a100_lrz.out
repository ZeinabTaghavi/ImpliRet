Starting the job
INFO 06-09 11:09:28 [__init__.py:243] Automatically detected platform cuda.
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_-1_1749460178.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 06-09 11:09:39 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-09 11:09:39 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-09 11:09:39 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-09 11:09:56 [config.py:793] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 06-09 11:09:56 [config.py:1875] Defaulting to use mp for distributed inference
INFO 06-09 11:09:56 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 06-09 11:09:59 [core.py:438] Waiting for init message from front-end.
INFO 06-09 11:09:59 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-09 11:09:59 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-09 11:09:59 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_d206ecd9'), local_subscribe_addr='ipc:///tmp/30d9ce60-81a8-46b2-9e8c-9b24728c88fe', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-09 11:10:01 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f759a2e2450>
WARNING 06-09 11:10:01 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f759a2e2a80>
WARNING 06-09 11:10:01 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f759a2e2480>
WARNING 06-09 11:10:01 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f759a2e23c0>
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:01 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d3242d9e'), local_subscribe_addr='ipc:///tmp/5df482d9-1a1f-4f22-978f-3269f606f1f2', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:01 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_001ff4b7'), local_subscribe_addr='ipc:///tmp/6c9cbaad-f5db-4606-9c04-4afc6502817f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:01 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6ae59fe1'), local_subscribe_addr='ipc:///tmp/929b773e-862e-490d-b269-aad0291f679d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:01 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5c226071'), local_subscribe_addr='ipc:///tmp/10ce8d30-a084-4de5-ad1a-6446e4903184', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:08 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:08 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:08 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:08 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:08 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:08 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:08 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:08 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:20 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:20 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:20 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:20 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:20 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c27dabb5'), local_subscribe_addr='ipc:///tmp/e63d5780-1e8f-4f35-93c8-c1990539ede4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:20 [parallel_state.py:1064] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:20 [parallel_state.py:1064] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:20 [parallel_state.py:1064] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:20 [parallel_state.py:1064] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m WARNING 06-09 11:10:21 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m WARNING 06-09 11:10:21 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m WARNING 06-09 11:10:21 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m WARNING 06-09 11:10:21 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:21 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:21 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:21 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:21 [gpu_model_runner.py:1531] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:22 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:22 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:22 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:22 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:22 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:22 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:22 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:22 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:10:23 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:10:23 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:10:23 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:10:23 [weight_utils.py:291] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:13:21 [default_loader.py:280] Loading weights took 177.33 seconds
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:13:21 [default_loader.py:280] Loading weights took 176.98 seconds
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:13:21 [default_loader.py:280] Loading weights took 177.71 seconds
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:13:21 [default_loader.py:280] Loading weights took 176.65 seconds
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:13:21 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 179.926788 seconds
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:13:21 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 179.904128 seconds
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:13:21 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 179.915120 seconds
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:13:21 [gpu_model_runner.py:1549] Model loading took 32.8894 GiB and 179.920448 seconds
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:13:40 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:13:40 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:13:40 [backends.py:469] Dynamo bytecode transform time: 19.01 s
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:13:40 [backends.py:469] Dynamo bytecode transform time: 19.01 s
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:13:45 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:13:45 [backends.py:469] Dynamo bytecode transform time: 23.33 s
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:13:45 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/9aad46b12e/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:13:45 [backends.py:469] Dynamo bytecode transform time: 23.59 s
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:13:47 [backends.py:158] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:13:48 [backends.py:158] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:13:50 [backends.py:158] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:13:51 [backends.py:158] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:14:44 [backends.py:170] Compiling a graph for general shape takes 61.17 s
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:14:51 [backends.py:170] Compiling a graph for general shape takes 69.21 s
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:14:52 [backends.py:170] Compiling a graph for general shape takes 65.05 s
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:15:01 [backends.py:170] Compiling a graph for general shape takes 75.00 s
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:15:49 [monitor.py:33] torch.compile takes 88.64 s in total
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:15:49 [monitor.py:33] torch.compile takes 98.33 s in total
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:15:49 [monitor.py:33] torch.compile takes 88.22 s in total
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:15:49 [monitor.py:33] torch.compile takes 80.18 s in total
INFO 06-09 11:16:00 [kv_cache_utils.py:637] GPU KV cache size: 566,016 tokens
INFO 06-09 11:16:00 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
INFO 06-09 11:16:00 [kv_cache_utils.py:637] GPU KV cache size: 563,360 tokens
INFO 06-09 11:16:00 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 11:16:00 [kv_cache_utils.py:637] GPU KV cache size: 563,360 tokens
INFO 06-09 11:16:00 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.30x
INFO 06-09 11:16:00 [kv_cache_utils.py:637] GPU KV cache size: 566,016 tokens
INFO 06-09 11:16:00 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 4.32x
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:16:47 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:16:49 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:16:52 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:16:54 [custom_all_reduce.py:195] Registering 10626 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=2634225)[0;0m INFO 06-09 11:16:56 [gpu_model_runner.py:1933] Graph capturing finished in 56 secs, took 4.77 GiB
[1;36m(VllmWorker rank=2 pid=2634224)[0;0m INFO 06-09 11:16:56 [gpu_model_runner.py:1933] Graph capturing finished in 56 secs, took 4.77 GiB
[1;36m(VllmWorker rank=0 pid=2634222)[0;0m INFO 06-09 11:16:56 [gpu_model_runner.py:1933] Graph capturing finished in 56 secs, took 4.77 GiB
[1;36m(VllmWorker rank=1 pid=2634223)[0;0m INFO 06-09 11:16:56 [gpu_model_runner.py:1933] Graph capturing finished in 57 secs, took 4.77 GiB
INFO 06-09 11:16:56 [core.py:167] init engine (profile, create kv cache, warmup model) took 214.90 seconds
Model loaded successfully.

 ----------- [STEP 4] Loading Tokenizer -----------
Tokenizer loaded successfully.
--------------------------------
Sending prompts to model...

 ----------- [STEP 1] Preparing Messages -----------

 ----------- [STEP 2] Setting Sampling Parameters -----------

 ----------- [STEP 3] Generating Responses -----------
INFO 06-09 11:17:02 [chat_utils.py:419] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
ERROR 06-09 11:22:38 [multiproc_executor.py:135] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
ERROR 06-09 11:27:37 [dump_input.py:68] Dumping input data
ERROR 06-09 11:27:37 [dump_input.py:70] V1 LLM engine (v0.9.0.1) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}, 
ERROR 06-09 11:27:37 [dump_input.py:78] Dumping scheduler output for model execution:
ERROR 06-09 11:27:37 [dump_input.py:79] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='17', resumed_from_preemption=false, new_token_ids=[11], new_block_ids=[[]], num_computed_tokens=17119), CachedRequestData(req_id='27', resumed_from_preemption=false, new_token_ids=[445], new_block_ids=[[]], num_computed_tokens=17109), CachedRequestData(req_id='49', resumed_from_preemption=false, new_token_ids=[7440], new_block_ids=[[10566]], num_computed_tokens=17248), CachedRequestData(req_id='78', resumed_from_preemption=false, new_token_ids=[914], new_block_ids=[[]], num_computed_tokens=17389), CachedRequestData(req_id='83', resumed_from_preemption=false, new_token_ids=[1109], new_block_ids=[[]], num_computed_tokens=17384), CachedRequestData(req_id='92', resumed_from_preemption=false, new_token_ids=[6017], new_block_ids=[[]], num_computed_tokens=16970), CachedRequestData(req_id='93', resumed_from_preemption=false, new_token_ids=[13], new_block_ids=[[]], num_computed_tokens=16969), CachedRequestData(req_id='96', resumed_from_preemption=false, new_token_ids=[402], new_block_ids=[[]], num_computed_tokens=16966), CachedRequestData(req_id='97', resumed_from_preemption=false, new_token_ids=[17], new_block_ids=[[]], num_computed_tokens=16965), CachedRequestData(req_id='98', resumed_from_preemption=false, new_token_ids=[5112], new_block_ids=[[]], num_computed_tokens=16961), CachedRequestData(req_id='99', resumed_from_preemption=false, new_token_ids=[8], new_block_ids=[[]], num_computed_tokens=16963), CachedRequestData(req_id='100', resumed_from_preemption=false, new_token_ids=[13], new_block_ids=[[]], num_computed_tokens=16961), CachedRequestData(req_id='101', resumed_from_preemption=false, new_token_ids=[400], new_block_ids=[[10567]], num_computed_tokens=16960), CachedRequestData(req_id='106', resumed_from_preemption=false, new_token_ids=[1233], new_block_ids=[[]], num_computed_tokens=16956), CachedRequestData(req_id='118', resumed_from_preemption=false, new_token_ids=[4815], new_block_ids=[[]], num_computed_tokens=16940), CachedRequestData(req_id='120', resumed_from_preemption=false, new_token_ids=[1101], new_block_ids=[[]], num_computed_tokens=17798), CachedRequestData(req_id='122', resumed_from_preemption=false, new_token_ids=[374], new_block_ids=[[]], num_computed_tokens=17796), CachedRequestData(req_id='127', resumed_from_preemption=false, new_token_ids=[72392], new_block_ids=[[]], num_computed_tokens=17789), CachedRequestData(req_id='145', resumed_from_preemption=false, new_token_ids=[374], new_block_ids=[[]], num_computed_tokens=17755), CachedRequestData(req_id='146', resumed_from_preemption=false, new_token_ids=[33874], new_block_ids=[[]], num_computed_tokens=17754), CachedRequestData(req_id='149', resumed_from_preemption=false, new_token_ids=[3074], new_block_ids=[[]], num_computed_tokens=17747), CachedRequestData(req_id='153', resumed_from_preemption=false, new_token_ids=[10845], new_block_ids=[[]], num_computed_tokens=17156), CachedRequestData(req_id='156', resumed_from_preemption=false, new_token_ids=[14874], new_block_ids=[[]], num_computed_tokens=17147), CachedRequestData(req_id='162', resumed_from_preemption=false, new_token_ids=[16217], new_block_ids=[[]], num_computed_tokens=17137), CachedRequestData(req_id='164', resumed_from_preemption=false, new_token_ids=[387], new_block_ids=[[]], num_computed_tokens=17134), CachedRequestData(req_id='167', resumed_from_preemption=false, new_token_ids=[2317], new_block_ids=[[]], num_computed_tokens=17130), CachedRequestData(req_id='169', resumed_from_preemption=false, new_token_ids=[11], new_block_ids=[[]], num_computed_tokens=17126), CachedRequestData(req_id='171', resumed_from_preemption=false, new_token_ids=[2753], new_block_ids=[[]], num_computed_tokens=17123), CachedRequestData(req_id='172', resumed_from_preemption=false, new_token_ids=[279], new_block_ids=[[]], num_computed_tokens=17116), CachedRequestData(req_id='174', resumed_from_preemption=false, new_token_ids=[4356], new_block_ids=[[]], num_computed_tokens=17115), CachedRequestData(req_id='175', resumed_from_preemption=false, new_token_ids=[539], new_block_ids=[[]], num_computed_tokens=17108), CachedRequestData(req_id='176', resumed_from_preemption=false, new_token_ids=[79447], new_block_ids=[[]], num_computed_tokens=17109), CachedRequestData(req_id='177', resumed_from_preemption=false, new_token_ids=[321], new_block_ids=[[]], num_computed_tokens=17108)], num_scheduled_tokens={100: 1, 127: 1, 146: 1, 120: 1, 153: 1, 177: 1, 93: 1, 97: 1, 98: 1, 49: 1, 118: 1, 149: 1, 167: 1, 78: 1, 171: 1, 122: 1, 162: 1, 17: 1, 164: 1, 92: 1, 145: 1, 156: 1, 174: 1, 96: 1, 27: 1, 169: 1, 176: 1, 175: 1, 172: 1, 106: 1, 99: 1, 101: 1, 83: 1}, total_num_scheduled_tokens=33, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[9], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
ERROR 06-09 11:27:37 [core.py:502] EngineCore encountered a fatal error.
ERROR 06-09 11:27:37 [core.py:502] Traceback (most recent call last):
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 215, in collective_rpc
ERROR 06-09 11:27:37 [core.py:502]     result = get_response(w, dequeue_timeout)
ERROR 06-09 11:27:37 [core.py:502]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 198, in get_response
ERROR 06-09 11:27:37 [core.py:502]     status, result = w.worker_response_mq.dequeue(
ERROR 06-09 11:27:37 [core.py:502]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 463, in dequeue
ERROR 06-09 11:27:37 [core.py:502]     with self.acquire_read(timeout, cancel) as buf:
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/contextlib.py", line 137, in __enter__
ERROR 06-09 11:27:37 [core.py:502]     return next(self.gen)
ERROR 06-09 11:27:37 [core.py:502]            ^^^^^^^^^^^^^^
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 427, in acquire_read
ERROR 06-09 11:27:37 [core.py:502]     raise TimeoutError
ERROR 06-09 11:27:37 [core.py:502] TimeoutError
ERROR 06-09 11:27:37 [core.py:502] 
ERROR 06-09 11:27:37 [core.py:502] The above exception was the direct cause of the following exception:
ERROR 06-09 11:27:37 [core.py:502] 
ERROR 06-09 11:27:37 [core.py:502] Traceback (most recent call last):
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 493, in run_engine_core
ERROR 06-09 11:27:37 [core.py:502]     engine_core.run_busy_loop()
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 520, in run_busy_loop
ERROR 06-09 11:27:37 [core.py:502]     self._process_engine_step()
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 545, in _process_engine_step
ERROR 06-09 11:27:37 [core.py:502]     outputs = self.step_fn()
ERROR 06-09 11:27:37 [core.py:502]               ^^^^^^^^^^^^^^
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 226, in step
ERROR 06-09 11:27:37 [core.py:502]     model_output = self.execute_model(scheduler_output)
ERROR 06-09 11:27:37 [core.py:502]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 213, in execute_model
ERROR 06-09 11:27:37 [core.py:502]     raise err
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 207, in execute_model
ERROR 06-09 11:27:37 [core.py:502]     return self.model_executor.execute_model(scheduler_output)
ERROR 06-09 11:27:37 [core.py:502]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 158, in execute_model
ERROR 06-09 11:27:37 [core.py:502]     (output, ) = self.collective_rpc("execute_model",
ERROR 06-09 11:27:37 [core.py:502]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-09 11:27:37 [core.py:502]   File "/dss/dsshome1/0B/di38wip/miniconda3/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 221, in collective_rpc
ERROR 06-09 11:27:37 [core.py:502]     raise TimeoutError(f"RPC call to {method} timed out.") from e
ERROR 06-09 11:27:37 [core.py:502] TimeoutError: RPC call to execute_model timed out.
INFO 06-09 11:27:49 [__init__.py:243] Automatically detected platform cuda.
[run_tests] Parsed CLI / YAML arguments
[run_tests] Found modelâ€‘config and dataset files.
[run_tests] Initialising ExperimentTester â€¦

 ----------- [STEP 1] Initialization -----------
Loading dataset configurations...

 ----------- [STEP 2] Processing Data -----------
Building conversation dictionaries...
[Init] ExperimentTester initialisation complete. Results will be written to ./Experiments/evaluation/results/A_Uni/llama_3.3_70b/A_Uni_llama_3.3_70b_1_1749461273.json

 ----------- [STEP 4] Running Evaluation -----------
Processing 1500 examples...

 ----------- [STEP 5] Generating Responses -----------
Loading model and passing prompts to it...

 ----------- [STEP 1] Checking vLLM Installation -----------

 ----------- [STEP 2] Setting Up Model Configuration -----------
[ModelLoader] tensor_parallel_size=4. Ensure you have enough GPU memory for all shards.
[ModelLoader] Loading meta-llama/Llama-3.3-70B-Instruct via local vLLM (TP=4, util=0.9, dir=None)

 ----------- [STEP 3] Loading Model -----------
INFO 06-09 11:27:53 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-09 11:27:53 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-09 11:27:53 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.

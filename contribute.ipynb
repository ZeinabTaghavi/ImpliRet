{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734691af",
   "metadata": {},
   "source": [
    "# ðŸ¤ Contribute to **ImpliRet**\n",
    "\n",
    "Thank you for contributing!  \n",
    "You can help in **two** ways:\n",
    "\n",
    "1. **Add a new retriever implementation** and open a PR.  \n",
    "2. **Submit readyâ€‘made retrieval results** so we can include them in the leaderboard.\n",
    "\n",
    "> If youâ€™re only interested in optionÂ 2, **skip StepÂ 1** below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c30b4e",
   "metadata": {},
   "source": [
    "## Stepâ€¯1 â€” Add a custom retriever  \n",
    "*(skip if you only want to upload results)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cdf07",
   "metadata": {},
   "source": [
    "Your retriever should be a **Python class** with two methods:\n",
    "\n",
    "```python\n",
    "class MyRetriever:\n",
    "    def __init__(self, corpus: list[str], k: int):\n",
    "        \"\"\"Store `corpus` and set the topâ€‘k youâ€™ll retrieve.\"\"\"\n",
    "\n",
    "    def retrieve_data(self, query: str) -> tuple[list[str], list[float]]:\n",
    "        \"\"\"Return *k* documents and their scores for the given `query`.\"\"\"\n",
    "```\n",
    "\n",
    "Below is a minimal example that wraps `rank_bm25`.  \n",
    "Feel free to swap in any library or algorithm!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53d75c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-2 results:\n",
      "1. doc= The iPhone features an improved camera system Â· score=1.3770\n",
      "2. doc= Apple unveils the new iPhone today Â· score=0.3462\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ Example skeleton â€” fill in the TODOs\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class MyBM25Retriever:\n",
    "    def __init__(self, corpus: List[str], k: int = 10):\n",
    "        self.k = k\n",
    "        self.corpus = corpus\n",
    "        tokenized = [doc.split() for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "    def retrieve_data(self, query: str) -> Tuple[List[str], List[float]]:\n",
    "        tokenized_q = query.split()\n",
    "        scores = np.array(self.bm25.get_scores(tokenized_q))\n",
    "        ranked_idx = scores.argsort()[::-1][:self.k]\n",
    "        return [self.corpus[i] for i in ranked_idx.tolist()], scores[ranked_idx].tolist()\n",
    "\n",
    "\n",
    "# â€”â€”â€” minimal working example â€”â€”â€”\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = [\n",
    "        \"Apple unveils the new iPhone today\",\n",
    "        \"Bananas are an excellent source of potassium\",\n",
    "        \"Python is a popular programming language\",\n",
    "        \"The iPhone features an improved camera system\",\n",
    "        \"Oranges are rich in vitamin C\"\n",
    "    ]\n",
    "\n",
    "    retriever = MyBM25Retriever(corpus, k=2)\n",
    "    idxs, scrs = retriever.retrieve_data(\"iPhone camera\")\n",
    "\n",
    "    print(\"Top-2 results:\")\n",
    "    for rank, (i, s) in enumerate(zip(idxs, scrs), 1):\n",
    "        print(f\"{rank}. doc= {i} Â· score={s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8621e7",
   "metadata": {},
   "source": [
    "### Where to put your code\n",
    "\n",
    "1. Save your file (for example content of previous cell) as `Retrieval/retrievals/MY_BM_retriever.py` (or another name).  \n",
    "2. Add an import branch to `Retrieval/retrieve_indexing.py` **linesâ€¯28â€“37**:\n",
    "\n",
    "```python\n",
    "if retriever_name.lower() == \"my_bm25\":\n",
    "        try:\n",
    "            from retrievals.MY_BM_retriever import MyBM25Retriever\n",
    "            retriever_module = MyBM25Retriever\n",
    "        except:\n",
    "            try:    \n",
    "                from Retrieval.retrievals.MY_BM_retriever import MyBM25Retriever\n",
    "                retriever_module = MyBM25Retriever\n",
    "            except:\n",
    "                raise Exception(\"MyBM25Retriever not found\")\n",
    "```\n",
    "\n",
    "3. Run the pipeline:\n",
    "\n",
    "```bash\n",
    "python Retrieval/retrieve_indexing.py \\\n",
    "       --output_folder Retrieval/results/ \\\n",
    "       --category arithmetic \\\n",
    "       --discourse multispeaker \\\n",
    "       --retriever_name my_bm25\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e2ad8",
   "metadata": {},
   "source": [
    "## Stepâ€¯2 â€” Provide results only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa9f96",
   "metadata": {},
   "source": [
    "For **each** of the six pools you must create a JSONL file named\n",
    "\n",
    "```\n",
    "{category}_{discourse}_{retriever_name}_index.jsonl\n",
    "# e.g. arithmetic_multispeaker_bm25_index.jsonl\n",
    "```\n",
    "\n",
    "* 1â€¯500 lines â€” exactly one per query  \n",
    "* Keys per line:\n",
    "\n",
    "| key | type | description |\n",
    "|-----|------|-------------|\n",
    "| `question` | str | The query text |\n",
    "| `gold_index` | int | Always the row index (groundâ€‘truth doc) |\n",
    "| `index_score_tuple_list` | list[[int, float]] | *kâ€¯â‰¥â€¯10* tuples of `(doc_index, score)` sorted by score (desc) |\n",
    "\n",
    "Example line:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What is the 2024 model price?\",\n",
    "  \"gold_index\": 0,\n",
    "  \"index_score_tuple_list\": [[273, 4.97], [102, 1.23], ...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f3cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arithmetic_multispeaker_bm25_index.jsonl: âœ” format looks good\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”Ž Validation helper\n",
    "import json, pathlib, sys\n",
    "\n",
    "def validate_jsonl(path, k_min=10, n_rows=1500):\n",
    "    path = pathlib.Path(path)\n",
    "    rows = path.read_text().splitlines()\n",
    "    assert len(rows) == n_rows, f\"{path}: expected {n_rows} rows, got {len(rows)}\"\n",
    "    for i, line in enumerate(rows):\n",
    "        data = json.loads(line)\n",
    "        assert set(data) >= {'question', 'gold_index', 'index_score_tuple_list'}, f\"{path}: missing keys on row {i}\"\n",
    "        lst = data['index_score_tuple_list']\n",
    "        assert len(lst) >= k_min, f\"row {i}: fewer than {k_min} retrieved docs\"\n",
    "        assert all(isinstance(t, list) and len(t)==2 for t in lst), f\"row {i}: each item must be [idx, score]\"\n",
    "    print(f\"{path.name}: âœ” format looks good\")\n",
    "\n",
    "# Example usage:\n",
    "# validate_jsonl('./Retrieval/results/arithmetic_multispeaker_bm25_index.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa50f8",
   "metadata": {},
   "source": [
    "### Submit\n",
    "\n",
    "* **Pull request** â€” fork the repo and add your code / JSONL files under `Experiments/evaluation/results`.  \n",
    "* **Email** â€” send the six JSONL files (and optionally your retriever code + `requirements.txt`) to **zeinabtaghavi1377@gmail.com**.\n",
    "\n",
    "Weâ€™ll run the validation script, merge, and your numbers will appear in the README leaderboard. ðŸŽ‰\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

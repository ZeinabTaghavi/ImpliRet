{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2d3a3d",
   "metadata": {},
   "source": [
    "# 🧪 ImpliRet · Hands-on Notebook\n",
    "\n",
    "This notebook is a **hands-on companion** to the ImpliRet benchmark  \n",
    "([📄 paper](https://arxiv.org/abs/2506.14407) • [💻 GitHub](https://github.com/ZeinabTaghavi/ImpliRet)).\n",
    "\n",
    "\n",
    "**In the next ~10&nbsp;minutes you will**\n",
    "\n",
    "1. 🔄 **Load** any of the six ImpliRet corpus slices  \n",
    "2. 🔍 **Run a single retriever** then score the results<br>(BM25, ColBERT-v2, Contriever, Dragon+, HippoRAG 2, ReasonIR-8B)  \n",
    "3. 📚 **RAG-Style** will give either the indexing result or oracle indexing into LLM(Llama-3, GPT-4.1, …)  \n",
    "\n",
    "> **Tip:** switch Colab to **GPU** (`Runtime → Change accelerator`) for a 3 × speed-up. We tested this notebook with RTX 2080 Ti GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2f1d1",
   "metadata": {},
   "source": [
    "🔍 First, let's check the number of available GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CUDA to utilize all available GPUs\n",
    "import torch\n",
    "import os\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    gpu_ids = list(range(num_gpus))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "    print(f\"Using {num_gpus} GPUs: {gpu_ids}\")\n",
    "else:\n",
    "    print(\"No GPUs detected, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad268ac",
   "metadata": {},
   "source": [
    "🔑 Hugging Face Login Required\n",
    "\n",
    "To use Llama models (in HippoRAG, ReasonIR, or RAG-Style experiments), you must first authenticate with Hugging Face.\n",
    "\n",
    "This allows access to model weights and tokenizers from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d93468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with Hugging Face using token\n",
    "hf_token = 'You_token_here'\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"Warning: HF_TOKEN environment variable not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e3011",
   "metadata": {},
   "source": [
    "🛠️ Clone the repository and install requirements\n",
    "\n",
    "First, we'll clone the ImpliRet repository and install the required dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2c6ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (≈ 1 min)\n",
    "# Python version should be >= 3.9\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEVICE = \"cuda\"  # Options: [\"cuda\", \"cpu\"]\n",
    "!git clone https://github.com/ZeinabTaghavi/ImpliRet.git\n",
    "os.chdir(\"ImpliRet\")\n",
    "!python -m venv impliret_env && source impliret_env/bin/activate\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import os, json, pathlib, itertools, pprint, time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f238e8",
   "metadata": {},
   "source": [
    "## 1 · 🔄 **Load a subset of Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d084c",
   "metadata": {},
   "source": [
    "Use 🤗 **datasets** to stream any subset on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e6007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,500 examples  |  Columns → ['id', 'tuple_set_id', 'forum_question', 'pos_document', 'question', 'answer', 'explicit_hint']\n",
      "\n",
      "Sample question → What brand and model of what was the brand and model of the smartphone that cost $1950? were priced at $1,950?\n",
      "Implicit document snippet → 2024-12-15 09:01, Zoe: I’m happy to share my thoughts on this. As a wildlife photographer constantly on the move, I understand the need for a reliable smartphone, and I recently upgraded myself with a ...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SPLIT = \"arithmetic\"       #@param [\"arithmetic\", \"wknow\", \"temporal\"]\n",
    "STYLE = \"multispeaker\"     #@param [\"multispeaker\", \"unispeaker\"]\n",
    "\n",
    "ds = load_dataset(\"zeinabTaghavi/ImpliRet\", name=STYLE, split=SPLIT)\n",
    "print(f\"Loaded {len(ds):,} examples  |  Columns → {list(ds.features.keys())}\")\n",
    "print(\"\\nSample question →\", ds[0][\"question\"])\n",
    "print(\"Implicit document snippet →\", ds[0][\"pos_document\"][:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692487d3",
   "metadata": {},
   "source": [
    "## 2 · 🔍 Run a single retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c70c09",
   "metadata": {},
   "source": [
    "You can **either** run the full baseline matrix via a helper **bash** script  \n",
    "*or* call the **Python CLI** for a single configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa78c41",
   "metadata": {},
   "source": [
    "### **Quick & Single-shot (bash)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f68dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the retrieval script\n",
    "! bash retrieve.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a434b6",
   "metadata": {},
   "source": [
    "### **Step by Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db93767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Retrieval.retrieve_indexing import save_retriever_indices\n",
    "\n",
    "OUTPUT = pathlib.Path(\"Retrieval/results\")\n",
    "OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RETRIEVER = \"bm25\"      #@param [\"bm25\", \"colbert\", \"contriever\", \"dragon_plus\", \"hipporag2\", \"reasonir8b\"]\n",
    "\n",
    "run_file = save_retriever_indices(\n",
    "    output_folder=str(OUTPUT),\n",
    "    category=SPLIT,\n",
    "    discourse=STYLE,\n",
    "    retriever_name=RETRIEVER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56837500",
   "metadata": {},
   "source": [
    "### 📊 Evaluate retrieval results\n",
    "\n",
    "Now lets go over evaluation!\n",
    "\n",
    "Thre evaluation result files also will be stored at `ImpliRet/Retrieval/reports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55fd28ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arithmetic | multispeaker | bm25: 1500 items\n",
      "Recall@10: 0.1627\n",
      "MRR@10: 0.0561\n",
      "nDCG@10: 0.0807\n"
     ]
    }
   ],
   "source": [
    "# Evaluate retrieval results and get recall, MRR and NDCG metrics\n",
    "from Retrieval.reporting import evaluate_run\n",
    "\n",
    "recall_results, mrr_results, ndcg_results = evaluate_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689becb2",
   "metadata": {},
   "source": [
    "## 3 · 📚 RAG-Style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa802a",
   "metadata": {},
   "source": [
    "You can **either** run the full baseline matrix via a helper **bash** script  \n",
    "*or* call the **Python CLI** for a single configuration. \n",
    "\n",
    "> Note: This example uses the lightweight [Llama 3.2 3B](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) model by default. You can configure other models in the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54abba",
   "metadata": {},
   "source": [
    "### **Quick & Single-shot (bash)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510b665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the job\n",
      "[run_tests] Parsed CLI / YAML arguments\n",
      "[run_tests] Found model-config.\n",
      "[run_tests] Initialising ExperimentTester …\n",
      "\n",
      " ----------- [STEP 1] Initialization -----------\n",
      "Loading dataset configurations...\n",
      "README.md: 7.00kB [00:00, 18.7MB/s]\n",
      "A_Multi.csv: 1.65MB [00:00, 7.85MB/s]\n",
      "W_Multi.csv: 1.67MB [00:00, 16.9MB/s]\n",
      "T_Multi.csv: 1.45MB [00:00, 19.8MB/s]\n",
      "Generating arithmetic split: 100%|█| 1500/1500 [00:00<00:00, 46455.75 examples/s\n",
      "Generating wknow split: 100%|█████| 1500/1500 [00:00<00:00, 40474.36 examples/s]\n",
      "Generating temporal split: 100%|██| 1500/1500 [00:00<00:00, 44631.02 examples/s]\n",
      "[Init] Loaded dataset with 1500 rows\n",
      "\n",
      " ----------- [STEP 2] Processing Data -----------\n",
      "Building conversation dictionaries...\n",
      "1500\n",
      "--------------------------------\n",
      "[Init] ExperimentTester initialisation complete. Results will be written to ./RAG_Style/results/arithmetic_multispeaker/llama_3.2_3b/arithmetic_multispeaker_llama_3.2_3b_1_1752586253.json\n",
      "\n",
      " ----------- [STEP 4] Running Evaluation -----------\n",
      "Processing 1500 examples...\n",
      "\n",
      " ----------- [STEP 5] Generating Responses -----------\n",
      "Loading model and passing prompts to it...\n",
      "\n",
      " ----------- [STEP 1] Checking vLLM Installation -----------\n",
      "\n",
      " ----------- [STEP 2] Setting Up Model Configuration -----------\n",
      "[ModelLoader] tensor_parallel_size=8. Ensure you have enough GPU memory for all shards.\n",
      "[ModelLoader] Loading meta-llama/Llama-3.2-3B-Instruct via local vLLM (TP=8, util=0.9, dir=None)\n",
      "\n",
      " ----------- [STEP 3] Loading Model -----------\n",
      "vllm_kwargs:\n",
      "{'model': 'meta-llama/Llama-3.2-3B-Instruct', 'download_dir': None, 'tensor_parallel_size': 8, 'gpu_memory_utilization': 0.9, 'max_model_len': 1024}\n",
      "----------\n",
      "INFO 07-15 15:31:02 config.py:510] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-15 15:31:02 config.py:1310] Defaulting to use mp for distributed inference\n",
      "INFO 07-15 15:31:02 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 07-15 15:31:03 multiproc_worker_utils.py:312] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-15 15:31:03 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 07-15 15:31:04 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-15 15:31:04 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m INFO 07-15 15:31:04 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m INFO 07-15 15:31:04 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m INFO 07-15 15:31:04 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m INFO 07-15 15:31:04 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m INFO 07-15 15:31:04 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m INFO 07-15 15:31:04 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m INFO 07-15 15:31:05 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m INFO 07-15 15:31:05 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m INFO 07-15 15:31:05 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m INFO 07-15 15:31:05 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m INFO 07-15 15:31:05 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m INFO 07-15 15:31:05 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m INFO 07-15 15:31:05 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m INFO 07-15 15:31:05 selector.py:129] Using XFormers backend.\n",
      "Bfloat16 not supported. Retrying with dtype='half'.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m INFO 07-15 15:31:07 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m INFO 07-15 15:31:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m INFO 07-15 15:31:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m INFO 07-15 15:31:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m INFO 07-15 15:31:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m INFO 07-15 15:31:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 140, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 479, in _check_if_gpu_supports_dtype\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 140, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 479, in _check_if_gpu_supports_dtype\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m ERROR 07-15 15:31:08 multiproc_worker_utils.py:236] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m INFO 07-15 15:31:08 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 140, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 479, in _check_if_gpu_supports_dtype\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 140, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 479, in _check_if_gpu_supports_dtype\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 140, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 479, in _check_if_gpu_supports_dtype\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 140, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 479, in _check_if_gpu_supports_dtype\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 140, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     _check_if_gpu_supports_dtype(self.model_config.dtype)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]   File \"/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/site-packages/vllm/worker/worker.py\", line 479, in _check_if_gpu_supports_dtype\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m ERROR 07-15 15:31:09 multiproc_worker_utils.py:236] ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "WARNING 07-15 15:31:09 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-15 15:31:09 config.py:510] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-15 15:31:09 config.py:1310] Defaulting to use mp for distributed inference\n",
      "INFO 07-15 15:31:09 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 07-15 15:31:10 multiproc_worker_utils.py:280] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/debugging.html#python-multiprocessing for more information.\n",
      "INFO 07-15 15:31:10 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-15 15:31:10 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:18 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:18 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:18 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:18 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:18 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:18 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:18 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:18 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:18 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:18 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:18 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:19 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:22 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:22 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:22 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:23 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:23 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:23 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:23 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:23 selector.py:129] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:23 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:24 utils.py:918] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:24 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-15 15:31:25 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 07-15 15:31:25 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_35828f50'), local_subscribe_port=34693, remote_subscribe_port=None)\n",
      "INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:25 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 07-15 15:31:25 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:25 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:25 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:25 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:25 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:25 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:25 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:26 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:05<00:05,  5.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.39s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.70s/it]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:33 model_runner.py:1099] Loading model weights took 0.7799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:39 worker.py:241] Memory profiling takes 6.09 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:39 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:39 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 8.08GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] Memory profiling takes 6.12 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 8.08GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] Memory profiling takes 6.14 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 8.08GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] Memory profiling takes 6.16 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 8.08GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] Memory profiling takes 6.17 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 8.08GiB.\n",
      "INFO 07-15 15:31:40 worker.py:241] Memory profiling takes 6.19 seconds\n",
      "INFO 07-15 15:31:40 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "INFO 07-15 15:31:40 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 7.13GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] Memory profiling takes 6.20 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 8.08GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] Memory profiling takes 6.36 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:40 worker.py:241] model weights take 0.78GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 8.08GiB.\n",
      "INFO 07-15 15:31:40 distributed_gpu_executor.py:57] # GPU blocks: 33398, # CPU blocks: 18724\n",
      "INFO 07-15 15:31:40 distributed_gpu_executor.py:61] Maximum concurrency for 1024 tokens per request: 521.84x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:31:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes:  97%|█████████████▌| 34/35 [00:25<00:00,  1.47it/s]\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:26<00:00,  1.30it/s]\n",
      "INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:32:11 model_runner.py:1535] Graph capturing finished in 27 secs, took 0.40 GiB\n",
      "INFO 07-15 15:32:11 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 37.92 seconds\n",
      "Model loaded successfully with dtype='half'.\n",
      "INFO 07-15 15:32:12 multiproc_worker_utils.py:140] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846081)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=846082)\u001b[0;0m INFO 07-15 15:32:12 multiproc_worker_utils.py:247] Worker exiting\n",
      "INFO 07-15 15:32:12 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846088)\u001b[0;0m INFO 07-15 15:32:12 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846085)\u001b[0;0m INFO 07-15 15:32:12 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846084)\u001b[0;0m INFO 07-15 15:32:12 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846087)\u001b[0;0m INFO 07-15 15:32:12 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846083)\u001b[0;0m INFO 07-15 15:32:12 multiproc_worker_utils.py:247] Worker exiting\n",
      "\n",
      " ----------- [STEP 4] Loading Tokenizer -----------\n",
      "Tokenizer loaded successfully.\n",
      "--------------------------------\n",
      "Sending prompts to model...\n",
      "INFO 07-15 15:32:13 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "Processed prompts: 100%|█| 1500/1500 [05:40<00:00,  4.40it/s, est. speed input: \n",
      "\n",
      " ----------- [STEP 6] Computing Scores -----------\n",
      "\n",
      " ----------- [STEP 7] Saving Results -----------\n",
      "\n",
      " ----------- Evaluation Complete -----------\n",
      "Results saved to: ./RAG_Style/results/arithmetic_multispeaker/llama_3.2_3b/arithmetic_multispeaker_llama_3.2_3b_1_1752586253.json\n",
      "[run_tests] Experiment completed.\n",
      "INFO 07-15 15:37:57 multiproc_worker_utils.py:140] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846167)\u001b[0;0m INFO 07-15 15:37:57 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846170)\u001b[0;0m INFO 07-15 15:37:57 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846168)\u001b[0;0m INFO 07-15 15:37:57 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846173)\u001b[0;0m INFO 07-15 15:37:57 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846171)\u001b[0;0m INFO 07-15 15:37:57 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846169)\u001b[0;0m INFO 07-15 15:37:57 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=846172)\u001b[0;0m INFO 07-15 15:37:57 multiproc_worker_utils.py:247] Worker exiting\n",
      "[rank0]:[W715 15:38:04.767064347 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "/mounts/Users/cisintern/zeinabtaghavi/miniconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "# Run the full baseline matrix via bash script\n",
    "! bash ./sync_run_tests.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef674b",
   "metadata": {},
   "source": [
    "### **Step by Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dddab774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_tests] Parsed CLI / YAML arguments\n",
      "[run_tests] Found model-config.\n",
      "[run_tests] Initialising ExperimentTester …\n",
      "\n",
      " ----------- [STEP 1] Initialization -----------\n",
      "Loading dataset configurations...\n",
      "[Init] Loaded dataset with 1500 rows\n",
      "\n",
      " ----------- [STEP 2] Processing Data -----------\n",
      "Building conversation dictionaries...\n",
      "1500\n",
      "--------------------------------\n",
      "[Init] ExperimentTester initialisation complete. Results will be written to RAG_Style/results/arithmetic_multispeaker/llama_3.2_3b/arithmetic_multispeaker_llama_3.2_3b_1_1750853307.json\n",
      "\n",
      " ----------- [STEP 4] Running Evaluation -----------\n",
      "Processing 1500 examples...\n",
      "\n",
      " ----------- [STEP 5] Generating Responses -----------\n",
      "Loading model and passing prompts to it...\n",
      "\n",
      " ----------- [STEP 1] Checking vLLM Installation -----------\n",
      "\n",
      " ----------- [STEP 2] Setting Up Model Configuration -----------\n",
      "[ModelLoader] Loading meta-llama/Llama-3.2-3B-Instruct via local vLLM (TP=1, util=0.9, dir=None)\n",
      "\n",
      " ----------- [STEP 3] Loading Model -----------\n",
      "vllm_kwargs:\n",
      "{'model': 'meta-llama/Llama-3.2-3B-Instruct', 'download_dir': None, 'tensor_parallel_size': 1, 'gpu_memory_utilization': 0.9, 'max_model_len': 1024}\n",
      "----------\n",
      "INFO 06-25 14:08:37 config.py:510] This model supports multiple tasks: {'embed', 'generate', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 06-25 14:08:37 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 06-25 14:08:39 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-25 14:08:39 selector.py:129] Using XFormers backend.\n",
      "Bfloat16 not supported. Retrying with dtype='half'.\n",
      "WARNING 06-25 14:08:41 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 06-25 14:08:41 config.py:510] This model supports multiple tasks: {'embed', 'generate', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 06-25 14:08:41 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 06-25 14:08:42 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-25 14:08:42 selector.py:129] Using XFormers backend.\n",
      "INFO 06-25 14:08:43 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 06-25 14:08:43 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cfc1d7920d4b7f88240bdeea975973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 14:08:52 model_runner.py:1099] Loading model weights took 6.0160 GB\n",
      "INFO 06-25 14:08:53 worker.py:241] Memory profiling takes 0.90 seconds\n",
      "INFO 06-25 14:08:53 worker.py:241] the current vLLM instance can use total_gpu_memory (10.57GiB) x gpu_memory_utilization (0.90) = 9.51GiB\n",
      "INFO 06-25 14:08:53 worker.py:241] model weights take 6.02GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 2.21GiB.\n",
      "INFO 06-25 14:08:53 gpu_executor.py:76] # GPU blocks: 1294, # CPU blocks: 2340\n",
      "INFO 06-25 14:08:53 gpu_executor.py:80] Maximum concurrency for 1024 tokens per request: 20.22x\n",
      "INFO 06-25 14:08:56 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 14:09:15 model_runner.py:1535] Graph capturing finished in 18 secs, took 0.19 GiB\n",
      "INFO 06-25 14:09:15 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with dtype='half'.\n",
      "\n",
      " ----------- [STEP 4] Loading Tokenizer -----------\n",
      "Tokenizer loaded successfully.\n",
      "--------------------------------\n",
      "Sending prompts to model...\n",
      "INFO 06-25 14:09:15 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  22%|██▏       | 336/1500 [00:27<01:25, 13.61it/s, est. speed input: 5039.65 toks/s, output: 346.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-25 14:09:45 scheduler.py:1555] Sequence group 381 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1500/1500 [02:00<00:00, 12.45it/s, est. speed input: 5113.30 toks/s, output: 441.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------- [STEP 6] Computing Scores -----------\n",
      "\n",
      " ----------- [STEP 7] Saving Results -----------\n",
      "\n",
      " ----------- Evaluation Complete -----------\n",
      "Results saved to: RAG_Style/results/arithmetic_multispeaker/llama_3.2_3b/arithmetic_multispeaker_llama_3.2_3b_1_1750853307.json\n",
      "[run_tests] Experiment completed.\n",
      "Saved generations → RAG_Style/results/arithmetic_multispeaker/llama_3.2_3b\n"
     ]
    }
   ],
   "source": [
    "from RAG_Style.scripts.syncr.sync_run_tests import run_experiment\n",
    "CONFIG = \"RAG_Style/experiment_configs/bm/A_Multi_llama_bm_10.yaml\"\n",
    "\n",
    "# Configure experiment parameters\n",
    "category = \"arithmetic\"              #@param [\"arithmetic\", \"wknowledge\", \"temporal\"] # Reasoning categories\n",
    "discourse_type = \"multispeaker\"      #@param [\"multispeaker\", \"unispeaker\"] # Forum vs chat style discourse\n",
    "\n",
    "# Configure model settings\n",
    "model_name = \"llama_3.2_3b\"         # Model name from vLLM-API config\n",
    "model_configs_dir = \"RAG_Style/model_configs\"\n",
    "\n",
    "# Configure evaluation settings\n",
    "metric = \"EM , contains , rouge-recall\"\n",
    "k = 1                               # Number of documents to retrieve\n",
    "use_retrieval = False               # Use user retrieval (True) or oracle retrieval (False)\n",
    "seed = 42\n",
    "output_folder = \"RAG_Style/results/\"\n",
    "\n",
    "# Run experiment with configured parameters\n",
    "result_path = run_experiment([\n",
    "    \"--model_name\",      model_name,\n",
    "    \"--model_configs_dir\", model_configs_dir,\n",
    "    \"--category\",        category,\n",
    "    \"--discourse\",       discourse_type,\n",
    "    \"--metric\",          \"EM,contains,rouge-recall\",   # no spaces\n",
    "    \"--k\",               str(k),                       # \"1\"\n",
    "    \"--use_retrieval\",   str(use_retrieval).lower(),   # \"false\"\n",
    "    \"--seed\",            str(seed),                    # \"42\"\n",
    "    \"--output_folder\",   output_folder\n",
    "])\n",
    "# Alternative: Use config file\n",
    "# result_path = run_experiment([\"--config\", CONFIG])\n",
    "\n",
    "print(\"Saved generations →\", result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de751f9",
   "metadata": {},
   "source": [
    "### 📊 Evaluate RAG-Style results\n",
    "\n",
    "Now lets go over evaluation!\n",
    "\n",
    "Thre evaluation result files also will be stored at `ImpliRet/RAG_Style/reports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a3c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed entries:\n",
      "Total entries: 2\n",
      "model_categories: dict_keys(['gpt-4.1-2025-04-14', 'llama_3.2_3b'])\n",
      "len(model_categories[list(model_categories.keys())[0]]): 1\n",
      "Result path: RAG_Style/results\n",
      "Metrics: ['EM', 'rouge-recall']\n",
      "Report output folder: RAG_Style/reports\n",
      "\n",
      "EM Table:\n",
      "Experiment | arithmetic | wknow | temporal | Uni Avg | Multi Avg \\\\\n",
      "K | unispeaker & multispeaker | unispeaker & multispeaker | unispeaker & multispeaker |  |  \\\\\n",
      "gpt-4.1-2025-04-14 & 1 & - & 9.00 & - & - & - & - & - & 9.00 \\\\\n",
      "llama_3.2_3b & 1 & - & 0.00 & - & - & - & - & - & 0.00 \\\\\n",
      "\n",
      "Average between unispeaker and multispeaker per k\n",
      "Experiment & K & Avg(Uni+Multi) \\\\\n",
      "gpt-4.1-2025-04-14 & 1 & 9.00 \\\\\n",
      "llama_3.2_3b & 1 & 0.00 \\\\\n",
      "\n",
      "\n",
      "ROUGE-1 Recall Table:\n",
      "Experiment | arithmetic | wknow | temporal | Uni Avg | Multi Avg \\\\\n",
      "K | unispeaker & multispeaker | unispeaker & multispeaker | unispeaker & multispeaker |  |  \\\\\n",
      "gpt-4.1-2025-04-14 & 1 & - & 93.87 & - & - & - & - & - & 93.87 \\\\\n",
      "llama_3.2_3b & 1 & - & 54.50 & - & - & - & - & - & 54.50 \\\\\n",
      "\n",
      "Average between unispeaker and multispeaker per k\n",
      "Experiment & K & Avg(Uni+Multi) \\\\\n",
      "gpt-4.1-2025-04-14 & 1 & 93.87 \\\\\n",
      "llama_3.2_3b & 1 & 54.50 \\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"ImpliRet\")\n",
    "from RAG_Style.reporting import reporting\n",
    "\n",
    "# Configure paths and parameters\n",
    "result_path = \"RAG_Style/results\"\n",
    "report_output_folder = \"RAG_Style/reports\"\n",
    "metrics = \"EM , rouge-recall\"\n",
    "reporting(result_path, metrics, report_output_folder, warn=False)\n",
    "\n",
    "# Print evaluation tables\n",
    "print(\"\\nEM Table:\")\n",
    "with open(\"RAG_Style/reports/EM_table.tex\", \"r\") as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\nROUGE-1 Recall Table:\")\n",
    "with open(\"RAG_Style/reports/rouge-1-recall_table.tex\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c449b",
   "metadata": {},
   "source": [
    "### 🎉 Thank you!\n",
    " \n",
    "Thanks for following along with this notebook! Hope it was helpful! 👋\n",
    "\n",
    "If you need help or think this notebook should be updated, feel free to email me at zeinabtaghavi1377@gmail.com 📧\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642fccb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

Starting the job
INFO 06-02 12:00:23 [__init__.py:243] Automatically detected platform cuda.
----- Experiment Setup ----
model_name: google/gemma-3-27b-it
track: A
conv_type: Multi
datasets_helping_folder: ./Dataset_Generation/Dataset_Helping
Number of GPUs: 4
---------------------------
Loading the model...
INFO 06-02 12:00:28 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-02 12:00:28 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-02 12:00:28 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-02 12:00:41 [config.py:793] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'.
INFO 06-02 12:00:41 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 06-02 12:00:45 [core.py:438] Waiting for init message from front-end.
INFO 06-02 12:00:45 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/dss/dssmcmlfs01/pn25pu/pn25pu-dss-0000/taghavi/HuggingFaceCache/', load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
INFO 06-02 12:01:18 [ray_utils.py:333] No current placement group found. Creating a new placement group.
INFO 06-02 12:01:19 [ray_distributed_executor.py:176] use_ray_spmd_worker: True
[36m(pid=4031679)[0m INFO 06-02 12:01:29 [__init__.py:243] Automatically detected platform cuda.
INFO 06-02 12:01:35 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
INFO 06-02 12:01:35 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']
INFO 06-02 12:01:35 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /dss/dsshome1/0B/di38wip/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:35 [__init__.py:31] Available plugins for group vllm.general_plugins:
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:35 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:35 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=4031697)[0m INFO 06-02 12:01:29 [__init__.py:243] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerWrapper pid=4031679)[0m WARNING 06-02 12:01:36 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6e5ddfb8c0>
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:42 [utils.py:1077] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:42 [pynccl.py:69] vLLM is using nccl==2.26.2
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:35 [__init__.py:31] Available plugins for group vllm.general_plugins:[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:35 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:35 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m WARNING 06-02 12:01:36 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f36d070b980>[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:44 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:44 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_1906377e'), local_subscribe_addr='ipc:///tmp/b2056517-f0a0-4d16-9cd3-c20190bf3f45', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:44 [parallel_state.py:1064] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(RayWorkerWrapper pid=4031697)[0m WARNING 06-02 12:01:56 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:42 [utils.py:1077] Found nccl from library libnccl.so.2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:42 [pynccl.py:69] vLLM is using nccl==2.26.2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:44 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /dss/dsshome1/0B/di38wip/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:44 [parallel_state.py:1064] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:57 [gpu_model_runner.py:1531] Starting to load model google/gemma-3-27b-it...
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:01:57 [cuda.py:217] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=4031680)[0m INFO 06-02 12:01:59 [backends.py:35] Using InductorAdaptor
[36m(RayWorkerWrapper pid=4031680)[0m INFO 06-02 12:01:59 [weight_utils.py:291] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:02:29 [default_loader.py:280] Loading weights took 28.95 seconds
[36m(RayWorkerWrapper pid=4031680)[0m WARNING 06-02 12:01:57 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:57 [gpu_model_runner.py:1531] Starting to load model google/gemma-3-27b-it...[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:57 [cuda.py:217] Using Flash Attention backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031689)[0m INFO 06-02 12:01:59 [backends.py:35] Using InductorAdaptor[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:01:59 [weight_utils.py:291] Using model weights format ['*.safetensors'][32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:02:29 [gpu_model_runner.py:1549] Model loading took 13.1667 GiB and 31.623979 seconds
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:02:29 [gpu_model_runner.py:1863] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
[36m(RayWorkerWrapper pid=4031689)[0m INFO 06-02 12:03:01 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/29d75a4ef6/rank_2_0 for vLLM's torch.compile
[36m(RayWorkerWrapper pid=4031689)[0m INFO 06-02 12:03:01 [backends.py:469] Dynamo bytecode transform time: 25.24 s
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:02:29 [default_loader.py:280] Loading weights took 28.45 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:02:29 [gpu_model_runner.py:1549] Model loading took 13.1667 GiB and 31.697928 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:02:29 [gpu_model_runner.py:1863] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031689)[0m INFO 06-02 12:03:26 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 23.530 s
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:03:03 [backends.py:459] Using cache directory: /dss/dsshome1/0B/di38wip/.cache/vllm/torch_compile_cache/29d75a4ef6/rank_0_0 for vLLM's torch.compile[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:03:03 [backends.py:469] Dynamo bytecode transform time: 27.50 s[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:03:38 [monitor.py:33] torch.compile takes 27.50 s in total
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:03:29 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 24.239 s[32m [repeated 3x across cluster][0m
INFO 06-02 12:03:41 [kv_cache_utils.py:637] GPU KV cache size: 483,200 tokens
INFO 06-02 12:03:41 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 117.97x
INFO 06-02 12:03:41 [kv_cache_utils.py:637] GPU KV cache size: 480,816 tokens
INFO 06-02 12:03:41 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 117.39x
INFO 06-02 12:03:41 [kv_cache_utils.py:637] GPU KV cache size: 483,200 tokens
INFO 06-02 12:03:41 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 117.97x
INFO 06-02 12:03:41 [kv_cache_utils.py:637] GPU KV cache size: 480,816 tokens
INFO 06-02 12:03:41 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 117.39x
[36m(RayWorkerWrapper pid=4031689)[0m INFO 06-02 12:04:29 [custom_all_reduce.py:195] Registering 8308 cuda graph addresses
[36m(RayWorkerWrapper pid=4031697)[0m INFO 06-02 12:03:38 [monitor.py:33] torch.compile takes 26.55 s in total[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4031679)[0m INFO 06-02 12:04:30 [gpu_model_runner.py:1933] Graph capturing finished in 49 secs, took 4.04 GiB
INFO 06-02 12:04:30 [core.py:167] init engine (profile, create kv cache, warmup model) took 120.82 seconds
INFO 06-02 12:04:43 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
Model loaded successfully using vLLM.
---------------------------------------- Generating the conversations ----------------------------------------
length of prompts_conversation_generation:  1500
Loading outputs from step 1...
len(conversation_list) 1500
Outputs loaded from ./Dataset_Generation/Dataset_Helping/A_Multi/A_Multi_Structured_Generated_conversation.jsonl and ./Dataset_Generation/Dataset_Helping/A_Multi/A_Multi_Structured_Generated_feature_extraction.jsonl
Number of mistaken conversations: 0
Number of mistaken extractions: 0
End of step 1
mistaken_extracted_idx: []
---------------------------------------- Dataset Generation Completed ----------------------------------------
Process group destroyed.
Job finished

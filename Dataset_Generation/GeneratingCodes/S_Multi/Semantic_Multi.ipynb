{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/zeinabtaghavi/MetatagIndexing_2/Dataset_Generation/GeneratingCodes/S_Multi\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"../../Dataset_Helping/names.txt\", \"r\") as file:\n",
    "    names = ' '.join([line.strip() for line in file.readlines()])\n",
    "    names_list = sorted(list(ast.literal_eval(names)))\n",
    "\n",
    "# Load the shopping data from the JSON file.\n",
    "# The file should have the structure as you described.\n",
    "with open(\"../../Dataset_Helping/S_Both/Wikidata_Final.jsonl\", \"r\") as f:\n",
    "    wikidata_samples = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "with open(\"../../Dataset_Helping/S_Multi/S_Multi_forum.jsonl\", \"r\") as f:\n",
    "    forum_topics = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untill you keep the country as a label\n",
    "for sample in wikidata_samples:\n",
    "    if 'country' not in sample.keys():\n",
    "        country = sample['label'].split('-')[-1].strip()\n",
    "        sample['country'] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item': 'Nkrumah University',\n",
       " 'label': 'Kabwe - Zambia',\n",
       " 'score': 0.17619562149047852,\n",
       " 'c4_count': 8527,\n",
       " 'country': 'Zambia'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikidata_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Solo backpacking essentials',\n",
       " 'forum_question': \"Hi everyone, I'm planning my first solo backpacking trip and could use some advice on what essentials to pack. Any tips for staying safe and traveling light?\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "\n",
    "def generate_user_trip_destination(country_list, num_items):\n",
    "    country_list_names = sorted(list(country_list.keys()))\n",
    "    country_list_names = random.sample(country_list_names, num_items)\n",
    "    user_trip_destinations = []\n",
    "    for country in country_list_names:\n",
    "        user_trip_destinations.append(random.choice(country_list[country]))\n",
    "\n",
    "    return user_trip_destinations\n",
    "\n",
    "\n",
    "def generate_message_for_each_user(list_of_users, list_of_destinations, year=2024):\n",
    "    messages = []\n",
    "    all_countries = []\n",
    "    for user, destination in zip(list_of_users, list_of_destinations):\n",
    "        random_date = f\"{year}-{random.randint(1,12)}-{random.randint(1,28)}\"\n",
    "        destination_name = destination['item']\n",
    "        destination_country = destination['country']\n",
    "        all_countries.append(destination_country)\n",
    "        message = {\n",
    "            \"forum_post\": (random_date, user, destination_name),\n",
    "            \"question\": f\"Who has ever been to {destination_country}?\",\n",
    "            \"answer\": user\n",
    "        }\n",
    "        \n",
    "        messages.append(message)\n",
    "    assert (len(list(set(all_countries))) == len(list_of_destinations))\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "shuffled_names_list = names_list.copy()\n",
    "num_items = 20\n",
    "\n",
    "country_list = {}\n",
    "for sample in wikidata_samples:\n",
    "    if sample['country'] not in country_list.keys():\n",
    "        country_list[sample['country']] = [sample]\n",
    "    else:\n",
    "        country_list[sample['country']].append(sample)\n",
    "\n",
    "# We'll generate enough unique users for each topic; here, 20 per topic.\n",
    "for topic in forum_topics:\n",
    "    dataset_row = {\n",
    "        'topic': topic[\"topic\"],\n",
    "        'forum_question': topic[\"forum_question\"],\n",
    "    }\n",
    "\n",
    "    list_of_users = random.sample(shuffled_names_list, num_items)\n",
    "    list_of_destinations = generate_user_trip_destination(country_list, num_items)\n",
    "    messages = generate_message_for_each_user(list_of_users, list_of_destinations)\n",
    "    dataset_row[\"posts\"] = messages\n",
    "    dataset.append(dataset_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: Arithmetic Multi Dataset Structure saved to ../../Dataset_Helping/S_Multi/S_Multi_Structured.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Save dataset\n",
    "output_dir = Path(\"../../Dataset_Helping/S_Multi\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / f\"S_Multi_Structured.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Step1: Arithmetic Multi Dataset Structure saved to {output_file}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 records from structured data file\n",
      "Loaded 500 records from generated data file\n",
      "- 7\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "user_response is '-'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     66\u001b[39m     base_path = \u001b[33m'\u001b[39m\u001b[33m../../../Dataset_Generation/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[43mmerging_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mmerging_dataset\u001b[39m\u001b[34m(base_path)\u001b[39m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m user_response == \u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28mprint\u001b[39m(user_response, i*\u001b[32m20\u001b[39m + j)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33muser_response is \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m         dataset.append({\n\u001b[32m     49\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser_ID\u001b[39m\u001b[33m\"\u001b[39m: i,\n\u001b[32m     50\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m: topic,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: posts[j][\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     57\u001b[39m         })\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(base_path + \u001b[33m'\u001b[39m\u001b[33mData/S_Multi.jsonl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mException\u001b[39m: user_response is '-'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "import json \n",
    "\n",
    "def merging_dataset(base_path):  \n",
    "    # Load the structured data\n",
    "    structured_data_path = base_path + \"Dataset_Helping/S_Multi/S_Multi_Structured.jsonl\"\n",
    "\n",
    "    with open(structured_data_path, 'r', encoding='utf-8') as f:\n",
    "        structured_data = [json.loads(line) for line in f]\n",
    "\n",
    "    print(f\"Loaded {len(structured_data)} records from structured data file\")\n",
    "\n",
    "    # Load the generated conversation data\n",
    "    generated_data_path = base_path + \"Dataset_Helping/S_Multi/S_Multi_Structured_Generated_conversation.jsonl\"\n",
    "\n",
    "    with open(generated_data_path, 'r', encoding='utf-8') as f:\n",
    "        generated_data = [json.loads(line) for line in f]\n",
    "\n",
    "    print(f\"Loaded {len(generated_data)} records from generated data file\")\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    x = 0\n",
    "    for i in range(len(structured_data)):\n",
    "        topic = structured_data[i]['topic']\n",
    "        forum_question = structured_data[i]['forum_question']\n",
    "        posts = structured_data[i]['posts']\n",
    "        for j in range(len(posts)):\n",
    "            message_date = posts[j]['forum_post'][0]\n",
    "            user = posts[j]['forum_post'][1]\n",
    "            if i*20 + j != x:\n",
    "                print(i*20 + j, x)\n",
    "            assert i*20 + j == x\n",
    "            x += 1\n",
    "            user_response = generated_data[int(i*20 + j)]\n",
    "            if user_response == '-':\n",
    "                print(user_response, i*20 + j)\n",
    "                raise Exception(\"user_response is '-'\")\n",
    "            dataset.append({\n",
    "                \"user_ID\": i,\n",
    "                \"topic\": topic,\n",
    "                \"forum_question\": forum_question,\n",
    "                \"message_date\": message_date,\n",
    "                \"user\": user,\n",
    "                \"user_response\": user_response,\n",
    "                \"question\": posts[j]['question'],\n",
    "                \"answer\": posts[j]['answer']\n",
    "            })\n",
    "\n",
    "    with open(base_path + 'Data/S_Multi.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(f\"A_Multi: {len(dataset)}, stored in {base_path}/Data/A_Multi.jsonl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = '../../../Dataset_Generation/'\n",
    "    merging_dataset(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

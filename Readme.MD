# ðŸ“š ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge


[![arXiv badge](https://img.shields.io/badge/arXiv-2506.14407-b31b1b.svg?logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.14407)
[![Hugging Face](https://img.shields.io/badge/%20Hugging%20Face-ImpliRet-ffca28?style=flat&logo=huggingface&logoColor=white)](https://huggingface.co/datasets/ZeinabTaghavi/ImpliRet)


**ImpliRet** (**Impli**cit Fact **Ret**rieval) is a six-subset benchmark that shifts the reasoning burden from
the *query* to the *documents themselves*.  
A query is always simple (e.g. *â€œWho visited a museum on October 06 2024?â€*) but the relevant document
contains the answer **only implicitly**â€”via:

| Reasoning category |    split   | Implicit cue inside the document | Query asks for â€¦ |
|--------------------|------------|----------------------------------|------------------|
| **Arithmetic**     |`arithmetic`|relative price or percentage (*â€œmodel 2019: 2.5 Ã— more expensiveâ€¦â€*) | absolute price |
| **Temporal**       |`temporal`  | relative date (*â€œseven days agoâ€*) | explicit date |
| **World Knowledge**|`wknow`     | landmark that implies a country (*â€œBig Benâ€ â‡’ UK*) | the person / item in that country |

For each category we generate two discourse styles:

|      Discourse   |      name     | style   |
|------------------|---------------|---------|
| **Multi Speaker**|`multispeaker` | Multi-speaker **forum thread** (one post â†”ï¸Ž one response)
| **Uni Speaker**  |`unispeaker`   | Uni-speaker **chat** (one-chat â†”ï¸Ž 10-turn dialogue)      

---

---
## ðŸ“ˆ Results

### ðŸ”¬ Retrieval Evaluatio

The table below reports **nDCG@10** (â†‘ higher is better) for our baseline retrievers.

| Retriever | W.Â Know. | Arithmetic | Temporal | Average |
|-----------|:-------:|:----------:|:--------:|:-------:|
| ***Sparse*** |  |  |  |  |
| BM25 | 14.10 | 11.06 | 11.22 | 12.13 |
| ***Late Interaction*** |  |  |  |  |
| ColBERTÂ v2 | 16.04 | 14.93 | 12.20 | 14.39 |
| ***Dense Encoders*** |  |  |  |  |
| Contriever | 16.15 | 13.84 | 12.59 | 14.19 |
| Dragon+ | 17.15 | **14.61** | 12.53 | 14.76 |
| ReasonIRâ€‘8B | **19.53** | 10.74 | **14.93** | **15.07** |
| ***Knowledgeâ€‘Graphâ€‘Augmented*** |  |  |  |  |
| HippoRAGÂ 2 | 16.38 | 14.21 | 12.50 | 14.36 |

*TableÂ 2. nDCG@10 retrieval performance averaged over uniâ€‘speaker and multiâ€‘speaker documents.*

### ðŸ§© RAGâ€‘style Evaluation

The table below shows **ROUGEâ€‘1 recall** (Râ€‘1@k) for two longâ€‘context LLM readers when the topâ€‘*k* retrieved documents (oracle setting) are supplied.

| Experiment | *k* | W.Â Know. | Arithmetic | Temporal | Average |
|------------|:---:|:--------:|:----------:|:--------:|:-------:|
| **LlamaÂ 3.3Â 70B** | 1   | **73.79** | **90.13** | **81.85** | **81.92** |
|                | 10  | 27.37 | 16.98 | 25.23 | 23.19 |
|                | All | 17.43 | 4.42  | 10.29 | 10.71 |
| **GPTâ€‘4.1**     | 1   | **93.24** | **92.12** | **84.90** | **88.05** |
|                | 10  | 62.21 | 23.86 | 15.59 | 35.06 |
|                | All | 53.91 | 9.28  |  6.93 | 22.90 |

*TableÂ 3. ROUGEâ€‘1 recall (Râ€‘1@k), averaged over uniâ€‘speaker and multiâ€‘speaker documents.*




---

## Evaluation Instruction



```bash
# clone & install
$ git clone https://github.com/ZeinabTaghavi/ImpliRet.git
$ cd ImpliRet
$ python -m venv impliret_env && source impliret_env/bin/activate
$ pip install -r requirements.txt
```

---

## Repository map

```
â”œâ”€â”€ RAG_Style/     
â”‚   â”œâ”€â”€ experiment_configs   # Config of RAG with retrievals or Oracle retriever
â”‚   â”œâ”€â”€ model_configs        # Config of each LLM that will be used in RAG_Style 
â”‚   â”œâ”€â”€ script               # Codes of Asyncron and Syncron experiments
â”‚   â”œâ”€â”€ results              
â”‚   â””â”€â”€ reports
â”œâ”€â”€ Retrieval/         
â”‚   â”œâ”€â”€ retrievals           # Codes of each experiment
â”‚   â”œâ”€â”€ results
â”‚   â””â”€â”€ reports
â””â”€â”€ README.md             
```

---

## 1. Loading the `ImpliRet` dataset.

You can load the dataset via Huggingface `zeinabTaghavi/ImpliRet` in the following format:
There are three reasoning categories (split): `['arithmetic', 'wknow', 'temporal']` and two discource style (name) `['multispeaker','unispeaker']`
```python
from datasets import load_dataset
# forum style, temporal reasoning
ds = load_dataset("zeinabTaghavi/ImpliRet",
                  name="multispeaker",
                  split="temporal")
print(ds[0]["question"])
print(ds[0]["pos_document"])
```

## 2. Evaluate retrieval baselines

Runing the retrievals for generating the indexes:
The retrievals: BM25s, ColBertV2, Contriever, DragonPlus, HippoRagV2, ReasonIR.

Example of running:
running the retriever and reporting.
```bash
bash Retrieval/retrieve_BM25s.bash
python Retrieval/reporting.py
```

Scores (MAP, nDCG\@10â€¦) are written to `./Retrieval/Results/A_Multi_bm25_index.jsonl`.
Reports are storing to `Retrieval/reports/retrieval_report.txt`.

âš ï¸ For runing HippoRAG2 and ReasonIR, we used 4 A100 GPUS.
---

## 3. Evaluate languageâ€‘model reasoning

Here we try Long context and RAG, the setting of the experiments configs are in the `Experiments/evaluation/run_configs` folder, the  config of models are also stored in `Experiments/evaluation/model_configs`.

```bash
# example with Llama3.3 70-B Turbo
# If you want to use OpenAI models, use your API-key, else if you want to load the VLLM on server, user any thing as API key (e.g., `EMPTY`).
export OPENAI_API_KEY=...
export HF_HOME=...

bash Experiments/evaluation/s_run_tests_bm.sh
# or for using server:
python Experiments/evaluation/async_evaluate.py \
  --config Experiments/evaluation/run_configs/bm/A_Multi_llama.yaml
# or for loading the vllm locally
python Experiments/evaluation/sync_evaluate.py \
  --config Experiments/evaluation/run_configs/bm/A_Multi_llama.yaml

# reporing the resutls:
python Experiments/evaluation/reporing.py
```

The outputs will be hashed and store in `Experiments/evaluation/results`


---

## Acknowledgements

Thank you so much!

# Implicit Reasoning EvaluationÂ Suite

> **Assess how well retrieval systems and largeâ€‘language models uncover facts that are *implied* rather than explicitly stated.**  
> This repository is a **threeâ€‘stage evaluation pipeline**â€”dataset generation, retrieval benchmarking, and LLM reasoningâ€”created for research on implicit information understanding.

---

##Â TableÂ ofÂ Contents
1. [QuickÂ Start](#quick-start)  
2. [DirectoryÂ Structure](#directory-structure)  
3. [StageÂ 1Â â€” DatasetÂ Generation](#stage-1--dataset-generation)  
4. [StageÂ 2Â â€” RetrievalÂ Evaluation](#stage-2--retrieval-evaluation)  
5. [StageÂ 3Â â€” LLMÂ Reasoning](#stage-3--llm-reasoning)  
6. [Acknowledgements](#acknowledgements)  
7. [License](#license)  

---

##Â QuickÂ Start
```bash
# Clone the repo
git clone https://github.com/ZeinabTaghavi/MetatagIndexing_2.git
cd MetatagIndexing_2

# Create and activate a virtual environment
python -m venv .venv && source .venv/bin/activate

# Install all Python dependencies
pip install -r requirements.txt
```
 
---

##Â DirectoryÂ Structure
```
Dataset_Generation/             # StageÂ 1Â â€” synthetic benchmark creation
â”‚   â”œâ”€â”€ Data/                   #   â–¶ final JSONL datasets
â”‚   â”œâ”€â”€ Dataset_Helping/        #   â–¶ auxiliary data files
â”‚   â””â”€â”€ GeneratingCodes/        #   â–¶ generation scripts (.py / .sh)
Experiments/                    # StageÂ 3Â â€” LLM evaluation
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ model_configs/      #   â–¶ HF / OpenAI model params
â”‚       â”œâ”€â”€ run_configs/        #   â–¶ experiment configs (YAML)
â”‚       â”œâ”€â”€ results/            #   â–¶ raw generations (JSONL)
â”‚       â””â”€â”€ reports/            #   â–¶ metric summaries (MD / JSON)
Retrieval/                      # StageÂ 2Â â€” retrieval baselines
â”‚   â”œâ”€â”€ Retrievals/             #   â–¶ indexers & searchers
â”‚   â”œâ”€â”€ Results/                #   â–¶ TREC run files
â”‚   â””â”€â”€ reports/                #   â–¶ IR metric summaries
README.md
```

---

##Â StageÂ 1Â â€” DatasetÂ Generation
We synthesise multiâ€‘turn conversations that *hide* key facts and cover three topical tracks:

| Track | Domain          | Example implicit relation                     |
|-------|-----------------|-----------------------------------------------|
| **A** | Arithmetic      | *â€œShe doubled the number six.â€ â‡’ 12*          |
| **T** | Temporal        | *â€œHe left two hours after noon.â€ â‡’ 14:00Â hrs* |
| **S** | Spatial         | *â€œThe cafÃ© is next to the library.â€*          |

For every **track** (`A`,Â `T`,Â `S`) and **audience style** (`Uni` chat vsÂ `Multi` forum) run:

```bash
# 1Â Generate structured triples
python Dataset_Generation/GeneratingCodes/A_Multi/Arithmetic_Multi_1_Structure_Gen.py

# 2Â Write conversations from the structures
bash   Dataset_Generation/GeneratingCodes/A_Multi/Arithmetic_Multi_2_Conversation_Gen.sh

# 3Â Merge to the final JSONL dataset
python Dataset_Generation/GeneratingCodes/A_Multi/Arithmetic_Multi_3_Merging.py
```

Generated files:

| File name         | Description                 |
|-------------------|-----------------------------|
| `A_Uni.jsonl`     | ArithmeticÂ Â· singleâ€‘speaker |
| `A_Multi.jsonl`   | ArithmeticÂ Â· multiâ€‘speaker  |
| `T_Uni.jsonl`     | TemporalÂ Â· singleâ€‘speaker   |
| `T_Multi.jsonl`   | TemporalÂ Â· multiâ€‘speaker    |
| `S_Uni.jsonl`     | SpatialÂ Â· singleâ€‘speaker    |
| `S_Multi.jsonl`   | SpatialÂ Â· multiâ€‘speaker     |

---

##Â StageÂ 2Â â€” RetrievalÂ Evaluation
We benchmark six retrievers: **BM25s, ColBERTâ€‘v2, Contriever, Dragonâ€‘Plus, HippoRAGâ€‘v2, ReasonIR**.

Example (BM25):

```bash
# Build index & produce rankings
bash Retrieval/retrieve_BM25s.bash

# Score the run (MAP,Â nDCG@10,Â Recall@100â€¦)
python Retrieval/reporting.py
```

*Outputs*  
- Rankings: `Retrieval/Results/A_Multi_bm25_index.jsonl`  
- Report:â€ƒ `Retrieval/reports/retrieval_report.txt`

> **GPU demand:** HippoRAGâ€‘v2 and ReasonIR were evaluated on 4Â Ã—Â A100Â (80Â GB) GPUs.

---

##Â StageÂ 3Â â€” LLMÂ Reasoning
We test longâ€‘context and RAG variants with configs in `Experiments/evaluation/run_configs`.

###Â Run with OpenAI (GPTâ€‘4o) or local VLLM (Llamaâ€‘3Â 70B)
```bash
# OpenAI endpoint
export OPENAI_API_KEY=<yourâ€‘key>

bash Experiments/evaluation/s_run_tests_bm.sh       # convenience wrapper

#Â â€” ORÂ â€” evaluate asynchronously on a remote VLLM server
python Experiments/evaluation/async_evaluate.py \
  --config Experiments/evaluation/run_configs/bm/A_Multi_llama.yaml

#Â â€” ORÂ â€” run locally in synchronous mode
python Experiments/evaluation/sync_evaluate.py \
  --config Experiments/evaluation/run_configs/bm/A_Multi_llama.yaml
```

Results are hashed and written to `Experiments/evaluation/results/`,  
with aggregate metrics in `Experiments/evaluation/reports/`.

---

##Â Acknowledgements
This project was made possible by the enthusiasm of the **ImplicitÂ Reasoning ReadingÂ Group** and infrastructure kindly provided by LRZ. 
LMU and MCML, Munich
---
